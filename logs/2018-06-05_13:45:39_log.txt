[1,   200] loss: 0.688603933
[1,   400] loss: 0.638861652
[1,   600] loss: 0.630257234
[1,   800] loss: 0.625130633
[1,  1000] loss: 0.619005087
[1,  1200] loss: 0.614513725
Final Summary:   loss: 6.365
Train Accuracy of the network: 0 %
[2,   200] loss: 0.633171716
[2,   400] loss: 0.632152961
[2,   600] loss: 0.632251387
[2,   800] loss: 0.632843390
[2,  1000] loss: 0.633543061
[2,  1200] loss: 0.632852860
Final Summary:   loss: 6.338
[3,   200] loss: 0.633348353
[3,   400] loss: 0.633366213
[3,   600] loss: 0.632302321
[3,   800] loss: 0.631566650
[3,  1000] loss: 0.631955799
[3,  1200] loss: 0.634984611
Final Summary:   loss: 6.339
[4,   200] loss: 0.635319307
[4,   400] loss: 0.632501023
[4,   600] loss: 0.631719148
[4,   800] loss: 0.631894733
[4,  1000] loss: 0.633212217
[4,  1200] loss: 0.633119317
Final Summary:   loss: 6.340
[5,   200] loss: 0.633403199
[5,   400] loss: 0.632356819
[5,   600] loss: 0.632728200
[5,   800] loss: 0.634989376
[5,  1000] loss: 0.633432694
[5,  1200] loss: 0.632419976
Final Summary:   loss: 6.343
[6,   200] loss: 0.631830601
[6,   400] loss: 0.631980464
[6,   600] loss: 0.632769408
[6,   800] loss: 0.633626147
[6,  1000] loss: 0.633339147
[6,  1200] loss: 0.633343291
Final Summary:   loss: 6.337
Train Accuracy of the network: 0 %
[7,   200] loss: 0.631740896
[7,   400] loss: 0.633400039
[7,   600] loss: 0.632643185
[7,   800] loss: 0.632693377
[7,  1000] loss: 0.633060922
[7,  1200] loss: 0.633420030
Final Summary:   loss: 6.339
[8,   200] loss: 0.632771532
[8,   400] loss: 0.634196862
[8,   600] loss: 0.634410929
[8,   800] loss: 0.633082573
[8,  1000] loss: 0.632524241
[8,  1200] loss: 0.631956222
Final Summary:   loss: 6.340
[9,   200] loss: 0.630576451
[9,   400] loss: 0.633065119
[9,   600] loss: 0.633686233
[9,   800] loss: 0.633451104
[9,  1000] loss: 0.633505667
[9,  1200] loss: 0.633048402
Final Summary:   loss: 6.337
[10,   200] loss: 0.631836451
[10,   400] loss: 0.633360741
[10,   600] loss: 0.633353388
[10,   800] loss: 0.634144889
[10,  1000] loss: 0.632012030
[10,  1200] loss: 0.633928581
Final Summary:   loss: 6.341
[11,   200] loss: 0.633238622
[11,   400] loss: 0.633010752
[11,   600] loss: 0.633432147
[11,   800] loss: 0.634234624
[11,  1000] loss: 0.632997917
[11,  1200] loss: 0.632408698
Final Summary:   loss: 6.342
Train Accuracy of the network: 0 %
[12,   200] loss: 0.632645944
[12,   400] loss: 0.632419005
[12,   600] loss: 0.632770930
[12,   800] loss: 0.632846230
[12,  1000] loss: 0.631830305
[12,  1200] loss: 0.632830012
Final Summary:   loss: 6.336
[13,   200] loss: 0.633035985
[13,   400] loss: 0.633280570
[13,   600] loss: 0.630497453
[13,   800] loss: 0.632270068
[13,  1000] loss: 0.632315597
[13,  1200] loss: 0.632418342
Final Summary:   loss: 6.335
[14,   200] loss: 0.632254864
[14,   400] loss: 0.631875689
[14,   600] loss: 0.632127405
[14,   800] loss: 0.630226264
[14,  1000] loss: 0.631099523
[14,  1200] loss: 0.631181312
Final Summary:   loss: 6.324
[15,   200] loss: 0.630384790
[15,   400] loss: 0.629979526
[15,   600] loss: 0.628354550
[15,   800] loss: 0.630150579
[15,  1000] loss: 0.628672852
[15,  1200] loss: 0.627491760
Final Summary:   loss: 6.302
[16,   200] loss: 0.627261921
[16,   400] loss: 0.627188044
[16,   600] loss: 0.628335581
[16,   800] loss: 0.626714611
[16,  1000] loss: 0.626214334
[16,  1200] loss: 0.624563149
Final Summary:   loss: 6.277
Train Accuracy of the network: 0 %
[17,   200] loss: 0.625567269
[17,   400] loss: 0.624688498
[17,   600] loss: 0.623398926
[17,   800] loss: 0.623175486
[17,  1000] loss: 0.623264289
[17,  1200] loss: 0.622514183
Final Summary:   loss: 6.246
[18,   200] loss: 0.622192774
[18,   400] loss: 0.621663017
[18,   600] loss: 0.621015568
[18,   800] loss: 0.619903847
[18,  1000] loss: 0.622456983
[18,  1200] loss: 0.620793805
Final Summary:   loss: 6.221
[19,   200] loss: 0.620595228
[19,   400] loss: 0.619803844
[19,   600] loss: 0.617557667
[19,   800] loss: 0.617541848
[19,  1000] loss: 0.618199191
[19,  1200] loss: 0.615940153
Final Summary:   loss: 6.192
[20,   200] loss: 0.617633101
[20,   400] loss: 0.616404099
[20,   600] loss: 0.616080147
[20,   800] loss: 0.616156852
[20,  1000] loss: 0.614496552
[20,  1200] loss: 0.614432191
Final Summary:   loss: 6.167
[21,   200] loss: 0.615926064
[21,   400] loss: 0.614904327
[21,   600] loss: 0.613807042
[21,   800] loss: 0.613527757
[21,  1000] loss: 0.612745620
[21,  1200] loss: 0.612542917
Final Summary:   loss: 6.146
Train Accuracy of the network: 0 %
[22,   200] loss: 0.612876903
[22,   400] loss: 0.611846758
[22,   600] loss: 0.610900961
[22,   800] loss: 0.611212278
[22,  1000] loss: 0.611448524
[22,  1200] loss: 0.611917566
Final Summary:   loss: 6.123
[23,   200] loss: 0.610261649
[23,   400] loss: 0.610886822
[23,   600] loss: 0.609207974
[23,   800] loss: 0.609978685
[23,  1000] loss: 0.610139287
[23,  1200] loss: 0.610324488
Final Summary:   loss: 6.109
[24,   200] loss: 0.609781180
[24,   400] loss: 0.610224333
[24,   600] loss: 0.607845349
[24,   800] loss: 0.608094162
[24,  1000] loss: 0.606563086
[24,  1200] loss: 0.608008283
Final Summary:   loss: 6.091
[25,   200] loss: 0.607208623
[25,   400] loss: 0.608310582
[25,   600] loss: 0.605873034
[25,   800] loss: 0.606972070
[25,  1000] loss: 0.606374082
[25,  1200] loss: 0.606976044
Final Summary:   loss: 6.075
[26,   200] loss: 0.605273785
[26,   400] loss: 0.605452080
[26,   600] loss: 0.605410285
[26,   800] loss: 0.605819672
[26,  1000] loss: 0.604835517
[26,  1200] loss: 0.606362360
Final Summary:   loss: 6.060
Train Accuracy of the network: 0 %
[27,   200] loss: 0.605070140
[27,   400] loss: 0.605240554
[27,   600] loss: 0.603236834
[27,   800] loss: 0.603110566
[27,  1000] loss: 0.603912512
[27,  1200] loss: 0.602400846
Final Summary:   loss: 6.044
[28,   200] loss: 0.602068178
[28,   400] loss: 0.603055765
[28,   600] loss: 0.601905142
[28,   800] loss: 0.601503601
[28,  1000] loss: 0.600445351
[28,  1200] loss: 0.603214013
Final Summary:   loss: 6.026
[29,   200] loss: 0.600992518
[29,   400] loss: 0.599463820
[29,   600] loss: 0.600605991
[29,   800] loss: 0.600601300
[29,  1000] loss: 0.601802393
[29,  1200] loss: 0.599322820
Final Summary:   loss: 6.009
[30,   200] loss: 0.599731367
[30,   400] loss: 0.599204800
[30,   600] loss: 0.599678285
[30,   800] loss: 0.597617063
[30,  1000] loss: 0.598313890
[30,  1200] loss: 0.597270214
Final Summary:   loss: 5.992
[31,   200] loss: 0.597863849
[31,   400] loss: 0.596017587
[31,   600] loss: 0.596216156
[31,   800] loss: 0.598242616
[31,  1000] loss: 0.595117797
[31,  1200] loss: 0.597734969
Final Summary:   loss: 5.974
Train Accuracy of the network: 0 %
[32,   200] loss: 0.595452761
[32,   400] loss: 0.595660379
[32,   600] loss: 0.593627726
[32,   800] loss: 0.595800448
[32,  1000] loss: 0.593671698
[32,  1200] loss: 0.594912594
Final Summary:   loss: 5.953
[33,   200] loss: 0.592173203
[33,   400] loss: 0.593666323
[33,   600] loss: 0.591836113
[33,   800] loss: 0.593496528
[33,  1000] loss: 0.591504226
[33,  1200] loss: 0.593176217
Final Summary:   loss: 5.932
[34,   200] loss: 0.591201831
[34,   400] loss: 0.588916284
[34,   600] loss: 0.591636788
[34,   800] loss: 0.590345502
[34,  1000] loss: 0.590876352
[34,  1200] loss: 0.590860825
Final Summary:   loss: 5.912
[35,   200] loss: 0.587829437
[35,   400] loss: 0.589714702
[35,   600] loss: 0.587688265
[35,   800] loss: 0.587448198
[35,  1000] loss: 0.589754908
[35,  1200] loss: 0.590099337
Final Summary:   loss: 5.892
[36,   200] loss: 0.587757789
[36,   400] loss: 0.587155150
[36,   600] loss: 0.586247064
[36,   800] loss: 0.588079890
[36,  1000] loss: 0.587142312
[36,  1200] loss: 0.585768883
Final Summary:   loss: 5.876
Train Accuracy of the network: 1 %
[37,   200] loss: 0.585027999
[37,   400] loss: 0.583550336
[37,   600] loss: 0.584136735
[37,   800] loss: 0.584405097
[37,  1000] loss: 0.581325618
[37,  1200] loss: 0.584038190
Final Summary:   loss: 5.842
[38,   200] loss: 0.581196788
[38,   400] loss: 0.581627735
[38,   600] loss: 0.584045353
[38,   800] loss: 0.581675223
[38,  1000] loss: 0.579360582
[38,  1200] loss: 0.580499275
Final Summary:   loss: 5.820
[39,   200] loss: 0.579687549
[39,   400] loss: 0.577851486
[39,   600] loss: 0.577725658
[39,   800] loss: 0.576896312
[39,  1000] loss: 0.577836183
[39,  1200] loss: 0.577892158
Final Summary:   loss: 5.785
[40,   200] loss: 0.573451712
[40,   400] loss: 0.576469469
[40,   600] loss: 0.573095020
[40,   800] loss: 0.574573556
[40,  1000] loss: 0.575128275
[40,  1200] loss: 0.573291925
Final Summary:   loss: 5.749
[41,   200] loss: 0.570395911
[41,   400] loss: 0.569784383
[41,   600] loss: 0.570236214
[41,   800] loss: 0.570010842
[41,  1000] loss: 0.572537477
[41,  1200] loss: 0.569397081
Final Summary:   loss: 5.709
Train Accuracy of the network: 1 %
[42,   200] loss: 0.566607096
[42,   400] loss: 0.567571062
[42,   600] loss: 0.566159900
[42,   800] loss: 0.562809222
[42,  1000] loss: 0.565122964
[42,  1200] loss: 0.567252353
Final Summary:   loss: 5.665
[43,   200] loss: 0.562051984
[43,   400] loss: 0.564158398
[43,   600] loss: 0.560912749
[43,   800] loss: 0.561415813
[43,  1000] loss: 0.560656857
[43,  1200] loss: 0.557579779
Final Summary:   loss: 5.616
[44,   200] loss: 0.557544535
[44,   400] loss: 0.557130573
[44,   600] loss: 0.555879613
[44,   800] loss: 0.554630696
[44,  1000] loss: 0.553134818
[44,  1200] loss: 0.553759823
Final Summary:   loss: 5.559
[45,   200] loss: 0.550955418
[45,   400] loss: 0.551399469
[45,   600] loss: 0.547424329
[45,   800] loss: 0.548688666
[45,  1000] loss: 0.548769623
[45,  1200] loss: 0.547484177
Final Summary:   loss: 5.495
[46,   200] loss: 0.546249422
[46,   400] loss: 0.545637147
[46,   600] loss: 0.543285464
[46,   800] loss: 0.541890554
[46,  1000] loss: 0.538769331
[46,  1200] loss: 0.539203554
Final Summary:   loss: 5.431
Train Accuracy of the network: 1 %
[47,   200] loss: 0.535402808
[47,   400] loss: 0.533099101
[47,   600] loss: 0.537205302
[47,   800] loss: 0.534719409
[47,  1000] loss: 0.535237503
[47,  1200] loss: 0.536004316
Final Summary:   loss: 5.358
[48,   200] loss: 0.528925826
[48,   400] loss: 0.532076466
[48,   600] loss: 0.528727639
[48,   800] loss: 0.529551950
[48,  1000] loss: 0.527364212
