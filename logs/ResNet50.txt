[1,   200] loss: 0.694856000
[1,   400] loss: 0.639804132
[1,   600] loss: 0.634564653
[1,   800] loss: 0.631767201
[1,  1000] loss: 0.629548063
[1,  1200] loss: 0.624105757
[1,  1400] loss: 0.625073927
[1,  1600] loss: 0.621822990
[1,  1800] loss: 0.620009266
[1,  2000] loss: 0.615487901
[1,  2200] loss: 0.616073722
[1,  2400] loss: 0.612395585
Final Summary:   loss: 6.307
Train Accuracy of the network: 0 %
[2,   200] loss: 0.611827390
[2,   400] loss: 0.614106022
[2,   600] loss: 0.612962934
[2,   800] loss: 0.612092499
[2,  1000] loss: 0.614581601
[2,  1200] loss: 0.613641891
[2,  1400] loss: 0.613495929
[2,  1600] loss: 0.612731728
[2,  1800] loss: 0.613593970
[2,  2000] loss: 0.613389467
[2,  2200] loss: 0.613793630
[2,  2400] loss: 0.613993585
Final Summary:   loss: 6.136
[3,   200] loss: 0.613462756
[3,   400] loss: 0.612994124
[3,   600] loss: 0.614020100
[3,   800] loss: 0.613445038
[3,  1000] loss: 0.614023981
[3,  1200] loss: 0.613651996
[3,  1400] loss: 0.613242914
[3,  1600] loss: 0.613458188
[3,  1800] loss: 0.612573471
[3,  2000] loss: 0.612546798
[3,  2200] loss: 0.613384976
[3,  2400] loss: 0.612746615
Final Summary:   loss: 6.136
[4,   200] loss: 0.613744020
[4,   400] loss: 0.613464917
[4,   600] loss: 0.612911962
[4,   800] loss: 0.611318689
[4,  1000] loss: 0.612707292
[4,  1200] loss: 0.614805400
[4,  1400] loss: 0.613583061
[4,  1600] loss: 0.612541184
[4,  1800] loss: 0.614930511
[4,  2000] loss: 0.614869960
[4,  2200] loss: 0.613794947
[4,  2400] loss: 0.613945163
Final Summary:   loss: 6.138
[5,   200] loss: 0.614088238
[5,   400] loss: 0.614586521
[5,   600] loss: 0.611656887
[5,   800] loss: 0.613187402
[5,  1000] loss: 0.612806254
[5,  1200] loss: 0.616035544
[5,  1400] loss: 0.613169238
[5,  1600] loss: 0.612257374
[5,  1800] loss: 0.614746620
[5,  2000] loss: 0.613476877
[5,  2200] loss: 0.613010941
[5,  2400] loss: 0.612937452
Final Summary:   loss: 6.138
[6,   200] loss: 0.614999877
[6,   400] loss: 0.614258494
[6,   600] loss: 0.611141340
[6,   800] loss: 0.615448248
[6,  1000] loss: 0.611913165
[6,  1200] loss: 0.613931413
[6,  1400] loss: 0.613223454
[6,  1600] loss: 0.613094770
[6,  1800] loss: 0.612263178
[6,  2000] loss: 0.612726057
[6,  2200] loss: 0.614648522
[6,  2400] loss: 0.613726558
Final Summary:   loss: 6.137
Train Accuracy of the network: 0 %
[7,   200] loss: 0.612882505
[7,   400] loss: 0.614584557
[7,   600] loss: 0.613015166
[7,   800] loss: 0.613377741
[7,  1000] loss: 0.612854015
[7,  1200] loss: 0.614717791
[7,  1400] loss: 0.613823901
[7,  1600] loss: 0.615016780
[7,  1800] loss: 0.612725464
[7,  2000] loss: 0.612335501
[7,  2200] loss: 0.612839446
[7,  2400] loss: 0.613120745
Final Summary:   loss: 6.137
[8,   200] loss: 0.612465811
[8,   400] loss: 0.613412568
[8,   600] loss: 0.614791719
[8,   800] loss: 0.613612578
[8,  1000] loss: 0.612602238
[8,  1200] loss: 0.615235955
[8,  1400] loss: 0.613434370
[8,  1600] loss: 0.614303555
[8,  1800] loss: 0.614024575
[8,  2000] loss: 0.613091779
[8,  2200] loss: 0.611476421
[8,  2400] loss: 0.612838048
Final Summary:   loss: 6.138
[9,   200] loss: 0.613167696
[9,   400] loss: 0.612609157
[9,   600] loss: 0.612692880
[9,   800] loss: 0.612948936
[9,  1000] loss: 0.615702871
[9,  1200] loss: 0.613258412
[9,  1400] loss: 0.612693442
[9,  1600] loss: 0.614194793
[9,  1800] loss: 0.613625828
[9,  2000] loss: 0.614708443
[9,  2200] loss: 0.614158755
[9,  2400] loss: 0.613083236
Final Summary:   loss: 6.140
[10,   200] loss: 0.612810460
[10,   400] loss: 0.613063190
[10,   600] loss: 0.614174906
[10,   800] loss: 0.613030171
[10,  1000] loss: 0.614694258
[10,  1200] loss: 0.613188370
[10,  1400] loss: 0.615955033
[10,  1600] loss: 0.612934265
[10,  1800] loss: 0.612979617
[10,  2000] loss: 0.612975882
[10,  2200] loss: 0.611721536
[10,  2400] loss: 0.614339011
Final Summary:   loss: 6.138
[11,   200] loss: 0.611742333
[11,   400] loss: 0.614288533
[11,   600] loss: 0.611857794
[11,   800] loss: 0.614966675
[11,  1000] loss: 0.614568123
[11,  1200] loss: 0.612560509
[11,  1400] loss: 0.612369650
[11,  1600] loss: 0.613018663
[11,  1800] loss: 0.613161172
[11,  2000] loss: 0.613786789
[11,  2200] loss: 0.613476064
[11,  2400] loss: 0.613104654
Final Summary:   loss: 6.135
Train Accuracy of the network: 0 %
[12,   200] loss: 0.612402566
[12,   400] loss: 0.614194790
[12,   600] loss: 0.615438132
[12,   800] loss: 0.612519667
[12,  1000] loss: 0.615206330
[12,  1200] loss: 0.611950668
[12,  1400] loss: 0.614374983
[12,  1600] loss: 0.611822847
[12,  1800] loss: 0.612228285
[12,  2000] loss: 0.613741130
[12,  2200] loss: 0.614081257
[12,  2400] loss: 0.612583096
Final Summary:   loss: 6.137
[13,   200] loss: 0.613367319
[13,   400] loss: 0.613822165
[13,   600] loss: 0.611922625
[13,   800] loss: 0.612421243
[13,  1000] loss: 0.611719824
[13,  1200] loss: 0.613011013
[13,  1400] loss: 0.612936258
[13,  1600] loss: 0.613988028
[13,  1800] loss: 0.615068063
[13,  2000] loss: 0.613769498
[13,  2200] loss: 0.614127751
[13,  2400] loss: 0.613616991
Final Summary:   loss: 6.136
[14,   200] loss: 0.612610982
[14,   400] loss: 0.612378012
[14,   600] loss: 0.613064336
[14,   800] loss: 0.612836538
[14,  1000] loss: 0.614756459
[14,  1200] loss: 0.614300668
[14,  1400] loss: 0.612789575
[14,  1600] loss: 0.613892207
[14,  1800] loss: 0.614051155
[14,  2000] loss: 0.612916559
[14,  2200] loss: 0.611352541
[14,  2400] loss: 0.613134387
Final Summary:   loss: 6.134
[15,   200] loss: 0.613185173
[15,   400] loss: 0.613838398
[15,   600] loss: 0.614024366
[15,   800] loss: 0.613640074
[15,  1000] loss: 0.612865424
[15,  1200] loss: 0.613318909
[15,  1400] loss: 0.614721767
[15,  1600] loss: 0.611257456
[15,  1800] loss: 0.610266513
[15,  2000] loss: 0.611578209
[15,  2200] loss: 0.613339266
[15,  2400] loss: 0.611794798
Final Summary:   loss: 6.131
[16,   200] loss: 0.613526208
[16,   400] loss: 0.612301857
[16,   600] loss: 0.611313537
[16,   800] loss: 0.611363389
[16,  1000] loss: 0.613100865
[16,  1200] loss: 0.614644782
[16,  1400] loss: 0.612804767
[16,  1600] loss: 0.610599451
[16,  1800] loss: 0.613923485
[16,  2000] loss: 0.611787150
[16,  2200] loss: 0.613161355
[16,  2400] loss: 0.612096656
Final Summary:   loss: 6.128
Train Accuracy of the network: 0 %
[17,   200] loss: 0.612094492
[17,   400] loss: 0.611665725
[17,   600] loss: 0.612700025
[17,   800] loss: 0.611711678
[17,  1000] loss: 0.611472167
[17,  1200] loss: 0.611009449
[17,  1400] loss: 0.611516484
[17,  1600] loss: 0.611482603
[17,  1800] loss: 0.610924543
[17,  2000] loss: 0.612118757
[17,  2200] loss: 0.610469297
[17,  2400] loss: 0.612394673
Final Summary:   loss: 6.119
[18,   200] loss: 0.610310628
[18,   400] loss: 0.611125275
[18,   600] loss: 0.611796206
[18,   800] loss: 0.611522125
[18,  1000] loss: 0.609544021
[18,  1200] loss: 0.612377337
[18,  1400] loss: 0.611958008
[18,  1600] loss: 0.609919354
[18,  1800] loss: 0.610016713
[18,  2000] loss: 0.610723576
[18,  2200] loss: 0.610590099
[18,  2400] loss: 0.611286814
Final Summary:   loss: 6.112
[19,   200] loss: 0.609844357
[19,   400] loss: 0.612622828
[19,   600] loss: 0.608461091
[19,   800] loss: 0.609401076
[19,  1000] loss: 0.609435953
[19,  1200] loss: 0.611120924
[19,  1400] loss: 0.610780463
[19,  1600] loss: 0.612223095
[19,  1800] loss: 0.608439815
[19,  2000] loss: 0.611228215
[19,  2200] loss: 0.610030965
[19,  2400] loss: 0.610862962
Final Summary:   loss: 6.106
[20,   200] loss: 0.611214700
[20,   400] loss: 0.609510987
[20,   600] loss: 0.608961730
[20,   800] loss: 0.610724780
[20,  1000] loss: 0.610385587
[20,  1200] loss: 0.609231749
[20,  1400] loss: 0.609446808
[20,  1600] loss: 0.606806618
[20,  1800] loss: 0.608435869
[20,  2000] loss: 0.609040549
[20,  2200] loss: 0.610287190
[20,  2400] loss: 0.610492425
Final Summary:   loss: 6.098
[21,   200] loss: 0.609409328
[21,   400] loss: 0.609928815
[21,   600] loss: 0.609797714
[21,   800] loss: 0.607890002
[21,  1000] loss: 0.608210103
[21,  1200] loss: 0.609546111
[21,  1400] loss: 0.607838193
[21,  1600] loss: 0.608490569
[21,  1800] loss: 0.607777755
[21,  2000] loss: 0.608718143
[21,  2200] loss: 0.609325500
[21,  2400] loss: 0.608249525
Final Summary:   loss: 6.091
Train Accuracy of the network: 0 %
[22,   200] loss: 0.609150363
[22,   400] loss: 0.607424558
[22,   600] loss: 0.607131602
[22,   800] loss: 0.608619455
[22,  1000] loss: 0.607023776
[22,  1200] loss: 0.608693267
[22,  1400] loss: 0.608469114
[22,  1600] loss: 0.608025260
[22,  1800] loss: 0.607351713
[22,  2000] loss: 0.607285691
[22,  2200] loss: 0.607588634
[22,  2400] loss: 0.606368042
Final Summary:   loss: 6.081
[23,   200] loss: 0.606993595
[23,   400] loss: 0.607455921
[23,   600] loss: 0.608072860
[23,   800] loss: 0.606892924
[23,  1000] loss: 0.607701372
[23,  1200] loss: 0.606250886
[23,  1400] loss: 0.605911538
[23,  1600] loss: 0.606730789
[23,  1800] loss: 0.606184320
[23,  2000] loss: 0.604569484
[23,  2200] loss: 0.605179022
[23,  2400] loss: 0.608665701
Final Summary:   loss: 6.070
[24,   200] loss: 0.606987156
[24,   400] loss: 0.603145100
[24,   600] loss: 0.605458429
[24,   800] loss: 0.604706327
[24,  1000] loss: 0.606216849
[24,  1200] loss: 0.604410911
[24,  1400] loss: 0.606898303
[24,  1600] loss: 0.605588036
[24,  1800] loss: 0.607141523
[24,  2000] loss: 0.605405172
[24,  2200] loss: 0.606893799
[24,  2400] loss: 0.605597692
Final Summary:   loss: 6.059
[25,   200] loss: 0.605076509
[25,   400] loss: 0.605405727
[25,   600] loss: 0.603278543
[25,   800] loss: 0.604532533
[25,  1000] loss: 0.606444596
[25,  1200] loss: 0.604807855
[25,  1400] loss: 0.605037799
[25,  1600] loss: 0.604521861
[25,  1800] loss: 0.604904310
[25,  2000] loss: 0.606753066
[25,  2200] loss: 0.604082558
[25,  2400] loss: 0.603047694
Final Summary:   loss: 6.051
[26,   200] loss: 0.604313473
[26,   400] loss: 0.602560513
[26,   600] loss: 0.603434694
[26,   800] loss: 0.605006938
[26,  1000] loss: 0.603301721
[26,  1200] loss: 0.602251964
[26,  1400] loss: 0.602762712
[26,  1600] loss: 0.601990067
[26,  1800] loss: 0.604640888
[26,  2000] loss: 0.604656332
[26,  2200] loss: 0.601463166
[26,  2400] loss: 0.603266617
Final Summary:   loss: 6.036
Train Accuracy of the network: 0 %
[27,   200] loss: 0.600462579
[27,   400] loss: 0.603331567
[27,   600] loss: 0.601216056
[27,   800] loss: 0.602485561
[27,  1000] loss: 0.604384482
[27,  1200] loss: 0.600604330
[27,  1400] loss: 0.602170621
[27,  1600] loss: 0.601875723
[27,  1800] loss: 0.602129580
[27,  2000] loss: 0.602843610
[27,  2200] loss: 0.601246816
[27,  2400] loss: 0.601470937
Final Summary:   loss: 6.023
[28,   200] loss: 0.599942452
[28,   400] loss: 0.599515865
[28,   600] loss: 0.599967304
[28,   800] loss: 0.601238620
[28,  1000] loss: 0.601900729
[28,  1200] loss: 0.600324599
[28,  1400] loss: 0.599370810
[28,  1600] loss: 0.600897732
[28,  1800] loss: 0.602271191
[28,  2000] loss: 0.602690829
[28,  2200] loss: 0.600084431
[28,  2400] loss: 0.601374138
Final Summary:   loss: 6.010
[29,   200] loss: 0.597519738
[29,   400] loss: 0.597526234
[29,   600] loss: 0.599523336
[29,   800] loss: 0.597857185
[29,  1000] loss: 0.598593233
[29,  1200] loss: 0.599068790
[29,  1400] loss: 0.599831005
[29,  1600] loss: 0.598611701
[29,  1800] loss: 0.599292645
[29,  2000] loss: 0.598218060
[29,  2200] loss: 0.600252030
[29,  2400] loss: 0.599220993
Final Summary:   loss: 5.991
[30,   200] loss: 0.598605156
[30,   400] loss: 0.598268206
[30,   600] loss: 0.596247847
[30,   800] loss: 0.595952721
[30,  1000] loss: 0.598341705
[30,  1200] loss: 0.598696430
[30,  1400] loss: 0.596623375
[30,  1600] loss: 0.596771310
[30,  1800] loss: 0.597818110
[30,  2000] loss: 0.595832344
[30,  2200] loss: 0.596847597
[30,  2400] loss: 0.597926987
Final Summary:   loss: 5.976
[31,   200] loss: 0.594280462
[31,   400] loss: 0.596874704
[31,   600] loss: 0.594978302
[31,   800] loss: 0.597194511
[31,  1000] loss: 0.595905833
[31,  1200] loss: 0.594354769
[31,  1400] loss: 0.596346464
[31,  1600] loss: 0.596239785
[31,  1800] loss: 0.593000872
[31,  2000] loss: 0.593170963
[31,  2200] loss: 0.596760204
[31,  2400] loss: 0.594374948
Final Summary:   loss: 5.956
Train Accuracy of the network: 0 %
[32,   200] loss: 0.593470776
[32,   400] loss: 0.592343930
[32,   600] loss: 0.595953777
[32,   800] loss: 0.593585540
[32,  1000] loss: 0.592459731
[32,  1200] loss: 0.592364632
[32,  1400] loss: 0.594743085
[32,  1600] loss: 0.592615123
[32,  1800] loss: 0.591296954
[32,  2000] loss: 0.595001722
[32,  2200] loss: 0.592481513
[32,  2400] loss: 0.595066137
Final Summary:   loss: 5.938
[33,   200] loss: 0.590260720
[33,   400] loss: 0.590437875
[33,   600] loss: 0.591022583
[33,   800] loss: 0.591514518
[33,  1000] loss: 0.593359765
[33,  1200] loss: 0.591459299
[33,  1400] loss: 0.592084851
[33,  1600] loss: 0.592289464
[33,  1800] loss: 0.591192000
[33,  2000] loss: 0.593373223
[33,  2200] loss: 0.588416014
[33,  2400] loss: 0.591998736
Final Summary:   loss: 5.917
[34,   200] loss: 0.588991501
[34,   400] loss: 0.590108037
[34,   600] loss: 0.589237957
[34,   800] loss: 0.590381815
[34,  1000] loss: 0.587641324
[34,  1200] loss: 0.588618336
[34,  1400] loss: 0.588342708
[34,  1600] loss: 0.591325461
[34,  1800] loss: 0.586640268
[34,  2000] loss: 0.589628599
[34,  2200] loss: 0.587354693
[34,  2400] loss: 0.587806010
Final Summary:   loss: 5.891
[35,   200] loss: 0.586644477
[35,   400] loss: 0.587228953
[35,   600] loss: 0.589111964
[35,   800] loss: 0.585696528
[35,  1000] loss: 0.588086336
[35,  1200] loss: 0.586404724
[35,  1400] loss: 0.586598678
[35,  1600] loss: 0.586920144
[35,  1800] loss: 0.585724338
[35,  2000] loss: 0.583733913
[35,  2200] loss: 0.584529810
[35,  2400] loss: 0.586611561
Final Summary:   loss: 5.867
[36,   200] loss: 0.583533350
[36,   400] loss: 0.583847152
[36,   600] loss: 0.581860862
[36,   800] loss: 0.583201535
[36,  1000] loss: 0.584253897
[36,  1200] loss: 0.581308520
[36,  1400] loss: 0.583293166
[36,  1600] loss: 0.583018838
[36,  1800] loss: 0.583200289
[36,  2000] loss: 0.581326503
[36,  2200] loss: 0.580197541
[36,  2400] loss: 0.580487398
Final Summary:   loss: 5.827
Train Accuracy of the network: 1 %
[37,   200] loss: 0.578822453
[37,   400] loss: 0.578391255
[37,   600] loss: 0.575223012
[37,   800] loss: 0.582078624
[37,  1000] loss: 0.578202307
[37,  1200] loss: 0.579183939
[37,  1400] loss: 0.578940559
[37,  1600] loss: 0.577875864
[37,  1800] loss: 0.576680190
[37,  2000] loss: 0.578765970
[37,  2200] loss: 0.575833392
[37,  2400] loss: 0.578407016
Final Summary:   loss: 5.784
[38,   200] loss: 0.574339088
[38,   400] loss: 0.575044690
[38,   600] loss: 0.577226246
[38,   800] loss: 0.572458064
[38,  1000] loss: 0.574565403
[38,  1200] loss: 0.576382471
[38,  1400] loss: 0.572828686
[38,  1600] loss: 0.573335473
[38,  1800] loss: 0.571521167
[38,  2000] loss: 0.571544985
[38,  2200] loss: 0.572716182
[38,  2400] loss: 0.570763876
Final Summary:   loss: 5.738
[39,   200] loss: 0.568448808
[39,   400] loss: 0.568964679
[39,   600] loss: 0.569099916
[39,   800] loss: 0.567292809
[39,  1000] loss: 0.570128925
[39,  1200] loss: 0.567321675
[39,  1400] loss: 0.568403384
[39,  1600] loss: 0.566692942
[39,  1800] loss: 0.569187776
[39,  2000] loss: 0.568720843
[39,  2200] loss: 0.569533092
[39,  2400] loss: 0.562456812
Final Summary:   loss: 5.682
[40,   200] loss: 0.564376078
[40,   400] loss: 0.564262333
[40,   600] loss: 0.561994550
[40,   800] loss: 0.563399370
[40,  1000] loss: 0.559910235
[40,  1200] loss: 0.560860124
[40,  1400] loss: 0.564167910
[40,  1600] loss: 0.563396097
[40,  1800] loss: 0.562626177
[40,  2000] loss: 0.562167404
[40,  2200] loss: 0.560779950
[40,  2400] loss: 0.561397644
Final Summary:   loss: 5.628
[41,   200] loss: 0.559236291
[41,   400] loss: 0.554671463
[41,   600] loss: 0.557123470
[41,   800] loss: 0.555237522
[41,  1000] loss: 0.560537374
[41,  1200] loss: 0.554404311
[41,  1400] loss: 0.558365065
[41,  1600] loss: 0.557381066
[41,  1800] loss: 0.557225623
[41,  2000] loss: 0.557048698
[41,  2200] loss: 0.553383981
[41,  2400] loss: 0.555459573
Final Summary:   loss: 5.569
Train Accuracy of the network: 2 %
[42,   200] loss: 0.552744497
[42,   400] loss: 0.550603260
[42,   600] loss: 0.552917368
[42,   800] loss: 0.552014108
[42,  1000] loss: 0.551919829
[42,  1200] loss: 0.550334718
[42,  1400] loss: 0.550614537
[42,  1600] loss: 0.548603608
[42,  1800] loss: 0.549987311
[42,  2000] loss: 0.548651008
[42,  2200] loss: 0.547445843
[42,  2400] loss: 0.551697570
Final Summary:   loss: 5.511
[43,   200] loss: 0.547357003
[43,   400] loss: 0.545610179
[43,   600] loss: 0.547881546
[43,   800] loss: 0.547836267
[43,  1000] loss: 0.543997524
[43,  1200] loss: 0.545302510
[43,  1400] loss: 0.543093550
[43,  1600] loss: 0.547796888
[43,  1800] loss: 0.544600000
[43,  2000] loss: 0.544270611
[43,  2200] loss: 0.540727802
[43,  2400] loss: 0.542480379
Final Summary:   loss: 5.454
[44,   200] loss: 0.539131637
[44,   400] loss: 0.538886691
[44,   600] loss: 0.540553307
[44,   800] loss: 0.536993211
[44,  1000] loss: 0.540626239
[44,  1200] loss: 0.534520482
[44,  1400] loss: 0.537492835
[44,  1600] loss: 0.538574748
[44,  1800] loss: 0.539795887
[44,  2000] loss: 0.539454775
[44,  2200] loss: 0.536313961
[44,  2400] loss: 0.532947231
Final Summary:   loss: 5.381
[45,   200] loss: 0.532124936
[45,   400] loss: 0.526750845
[45,   600] loss: 0.534017605
[45,   800] loss: 0.529836794
[45,  1000] loss: 0.535497406
[45,  1200] loss: 0.529993225
[45,  1400] loss: 0.530474572
[45,  1600] loss: 0.528009664
[45,  1800] loss: 0.530309473
[45,  2000] loss: 0.528004802
[45,  2200] loss: 0.529138200
[45,  2400] loss: 0.524018872
Final Summary:   loss: 5.301
[46,   200] loss: 0.526003608
[46,   400] loss: 0.524286101
[46,   600] loss: 0.524567251
[46,   800] loss: 0.524329564
[46,  1000] loss: 0.519023896
[46,  1200] loss: 0.523739940
[46,  1400] loss: 0.520150621
[46,  1600] loss: 0.519272692
[46,  1800] loss: 0.519663523
[46,  2000] loss: 0.515768599
[46,  2200] loss: 0.517758467
[46,  2400] loss: 0.513966067
Final Summary:   loss: 5.209
Train Accuracy of the network: 3 %
[47,   200] loss: 0.509041622
[47,   400] loss: 0.514843958
[47,   600] loss: 0.511133300
[47,   800] loss: 0.512087770
[47,  1000] loss: 0.512527204
[47,  1200] loss: 0.510688211
[47,  1400] loss: 0.508843792
[47,  1600] loss: 0.507933162
[47,  1800] loss: 0.512240594
[47,  2000] loss: 0.508234946
[47,  2200] loss: 0.511188946
[47,  2400] loss: 0.505248601
Final Summary:   loss: 5.106
[48,   200] loss: 0.502949300
[48,   400] loss: 0.503863274
[48,   600] loss: 0.500698448
[48,   800] loss: 0.498917827
[48,  1000] loss: 0.502738210
[48,  1200] loss: 0.497428447
[48,  1400] loss: 0.499595855
[48,  1600] loss: 0.495102037
[48,  1800] loss: 0.499065761
[48,  2000] loss: 0.500815444
[48,  2200] loss: 0.497055376
[48,  2400] loss: 0.497030126
Final Summary:   loss: 4.998
[49,   200] loss: 0.489924398
[49,   400] loss: 0.485184103
[49,   600] loss: 0.491658981
[49,   800] loss: 0.489292457
[49,  1000] loss: 0.486837121
[49,  1200] loss: 0.489293234
[49,  1400] loss: 0.486239427
[49,  1600] loss: 0.490539828
[49,  1800] loss: 0.486461615
[49,  2000] loss: 0.486684718
[49,  2200] loss: 0.483554244
[49,  2400] loss: 0.482802884
Final Summary:   loss: 4.875
[50,   200] loss: 0.477020409
[50,   400] loss: 0.477607340
[50,   600] loss: 0.478868146
[50,   800] loss: 0.474030814
[50,  1000] loss: 0.475857797
[50,  1200] loss: 0.476871380
[50,  1400] loss: 0.477944024
[50,  1600] loss: 0.473161044
[50,  1800] loss: 0.474096276
[50,  2000] loss: 0.469508373
[50,  2200] loss: 0.473131692
[50,  2400] loss: 0.470406810
Final Summary:   loss: 4.751
[51,   200] loss: 0.468405893
[51,   400] loss: 0.467356602
[51,   600] loss: 0.459735210
[51,   800] loss: 0.464438165
[51,  1000] loss: 0.455053407
[51,  1200] loss: 0.462092514
[51,  1400] loss: 0.462603058
[51,  1600] loss: 0.461667135
[51,  1800] loss: 0.458758407
[51,  2000] loss: 0.458688842
[51,  2200] loss: 0.453931894
[51,  2400] loss: 0.460525257
Final Summary:   loss: 4.613
Train Accuracy of the network: 8 %
[52,   200] loss: 0.450348939
[52,   400] loss: 0.447557302
[52,   600] loss: 0.448889248
[52,   800] loss: 0.450246909
[52,  1000] loss: 0.442503274
[52,  1200] loss: 0.446307561
[52,  1400] loss: 0.443151494
[52,  1600] loss: 0.445015601
[52,  1800] loss: 0.443889439
[52,  2000] loss: 0.445047691
[52,  2200] loss: 0.444726067
[52,  2400] loss: 0.439609262
Final Summary:   loss: 4.457
[53,   200] loss: 0.434035736
[53,   400] loss: 0.429474229
[53,   600] loss: 0.432926550
[53,   800] loss: 0.431928753
[53,  1000] loss: 0.430397190
[53,  1200] loss: 0.426182428
[53,  1400] loss: 0.430277936
[53,  1600] loss: 0.431093451
[53,  1800] loss: 0.428145827
[53,  2000] loss: 0.429575836
[53,  2200] loss: 0.426622500
[53,  2400] loss: 0.427991483
Final Summary:   loss: 4.300
[54,   200] loss: 0.417225896
[54,   400] loss: 0.418808714
[54,   600] loss: 0.420180286
[54,   800] loss: 0.415164807
[54,  1000] loss: 0.416213255
[54,  1200] loss: 0.416959947
[54,  1400] loss: 0.406373185
[54,  1600] loss: 0.416961789
[54,  1800] loss: 0.408119969
[54,  2000] loss: 0.414005895
[54,  2200] loss: 0.410348427
[54,  2400] loss: 0.410285751
Final Summary:   loss: 4.144
[55,   200] loss: 0.401391187
[55,   400] loss: 0.401598760
[55,   600] loss: 0.404616905
[55,   800] loss: 0.405681529
[55,  1000] loss: 0.401130384
[55,  1200] loss: 0.402725657
[55,  1400] loss: 0.393112945
[55,  1600] loss: 0.402922892
[55,  1800] loss: 0.397254125
[55,  2000] loss: 0.402334351
[55,  2200] loss: 0.396264556
[55,  2400] loss: 0.397458241
Final Summary:   loss: 4.007
[56,   200] loss: 0.380734581
[56,   400] loss: 0.383140109
[56,   600] loss: 0.388998961
[56,   800] loss: 0.382671751
[56,  1000] loss: 0.385702211
[56,  1200] loss: 0.385114628
[56,  1400] loss: 0.388638287
[56,  1600] loss: 0.385293388
[56,  1800] loss: 0.383875455
[56,  2000] loss: 0.389722809
[56,  2200] loss: 0.383675524
[56,  2400] loss: 0.383016438
Final Summary:   loss: 3.852
Train Accuracy of the network: 15 %
[57,   200] loss: 0.375700913
[57,   400] loss: 0.369244104
[57,   600] loss: 0.373599986
[57,   800] loss: 0.366597159
[57,  1000] loss: 0.378079416
[57,  1200] loss: 0.367101695
[57,  1400] loss: 0.369183241
[57,  1600] loss: 0.358035706
[57,  1800] loss: 0.371433842
[57,  2000] loss: 0.374634294
[57,  2200] loss: 0.367813008
[57,  2400] loss: 0.374332851
Final Summary:   loss: 3.707
[58,   200] loss: 0.359305179
[58,   400] loss: 0.358725701
[58,   600] loss: 0.363838262
[58,   800] loss: 0.350504540
[58,  1000] loss: 0.356558985
[58,  1200] loss: 0.358868422
[58,  1400] loss: 0.353894437
[58,  1600] loss: 0.356375695
[58,  1800] loss: 0.354714484
[58,  2000] loss: 0.354703042
[58,  2200] loss: 0.356296640
[58,  2400] loss: 0.361707956
Final Summary:   loss: 3.574
[59,   200] loss: 0.345177468
[59,   400] loss: 0.339992224
[59,   600] loss: 0.346843532
[59,   800] loss: 0.348674227
[59,  1000] loss: 0.343405529
[59,  1200] loss: 0.341234248
[59,  1400] loss: 0.348065759
[59,  1600] loss: 0.342717247
[59,  1800] loss: 0.344106835
[59,  2000] loss: 0.341983735
[59,  2200] loss: 0.335390911
[59,  2400] loss: 0.341102545
Final Summary:   loss: 3.434
[60,   200] loss: 0.335750069
[60,   400] loss: 0.332181915
[60,   600] loss: 0.330831772
[60,   800] loss: 0.328639051
[60,  1000] loss: 0.328811647
[60,  1200] loss: 0.323824675
[60,  1400] loss: 0.325542052
[60,  1600] loss: 0.333726066
[60,  1800] loss: 0.330935176
[60,  2000] loss: 0.324856845
[60,  2200] loss: 0.328454767
[60,  2400] loss: 0.327129689
Final Summary:   loss: 3.294
[61,   200] loss: 0.319022844
[61,   400] loss: 0.315577002
[61,   600] loss: 0.318161640
[61,   800] loss: 0.314384313
[61,  1000] loss: 0.317264347
[61,  1200] loss: 0.305526037
[61,  1400] loss: 0.319331817
[61,  1600] loss: 0.310131119
[61,  1800] loss: 0.311445504
[61,  2000] loss: 0.313112768
[61,  2200] loss: 0.316531406
[61,  2400] loss: 0.316401144
Final Summary:   loss: 3.150
Train Accuracy of the network: 26 %
[62,   200] loss: 0.303413978
[62,   400] loss: 0.302607181
[62,   600] loss: 0.295349772
[62,   800] loss: 0.299660625
[62,  1000] loss: 0.301338796
[62,  1200] loss: 0.308963850
[62,  1400] loss: 0.298931518
[62,  1600] loss: 0.305991689
[62,  1800] loss: 0.306193085
[62,  2000] loss: 0.294335634
[62,  2200] loss: 0.302713811
[62,  2400] loss: 0.294688934
Final Summary:   loss: 3.016
[63,   200] loss: 0.292521880
[63,   400] loss: 0.291539278
[63,   600] loss: 0.290373704
[63,   800] loss: 0.280619446
[63,  1000] loss: 0.290494240
[63,  1200] loss: 0.295425961
[63,  1400] loss: 0.294595193
[63,  1600] loss: 0.286934708
[63,  1800] loss: 0.284148802
[63,  2000] loss: 0.289930438
[63,  2200] loss: 0.284153708
[63,  2400] loss: 0.287413493
Final Summary:   loss: 2.893
[64,   200] loss: 0.279916300
[64,   400] loss: 0.278808164
[64,   600] loss: 0.279396274
[64,   800] loss: 0.277565429
[64,  1000] loss: 0.275528678
[64,  1200] loss: 0.278111025
[64,  1400] loss: 0.283137743
[64,  1600] loss: 0.276720024
[64,  1800] loss: 0.272842842
[64,  2000] loss: 0.271119388
[64,  2200] loss: 0.276822997
[64,  2400] loss: 0.269787654
Final Summary:   loss: 2.769
[65,   200] loss: 0.268016278
[65,   400] loss: 0.256908211
[65,   600] loss: 0.270189327
[65,   800] loss: 0.265972842
[65,  1000] loss: 0.272532849
[65,  1200] loss: 0.265260807
[65,  1400] loss: 0.260529451
[65,  1600] loss: 0.269015281
[65,  1800] loss: 0.255492671
[65,  2000] loss: 0.261016560
[65,  2200] loss: 0.270106000
[65,  2400] loss: 0.266001680
Final Summary:   loss: 2.655
[66,   200] loss: 0.243593861
[66,   400] loss: 0.254804674
[66,   600] loss: 0.257756694
[66,   800] loss: 0.254735493
[66,  1000] loss: 0.251279213
[66,  1200] loss: 0.257357375
[66,  1400] loss: 0.255086613
[66,  1600] loss: 0.254899905
[66,  1800] loss: 0.252030704
[66,  2000] loss: 0.252546826
[66,  2200] loss: 0.254598031
[66,  2400] loss: 0.251152997
Final Summary:   loss: 2.537
Train Accuracy of the network: 33 %
[67,   200] loss: 0.242880105
[67,   400] loss: 0.242030045
[67,   600] loss: 0.244785826
[67,   800] loss: 0.244798025
[67,  1000] loss: 0.249360095
[67,  1200] loss: 0.244273551
[67,  1400] loss: 0.240408026
[67,  1600] loss: 0.239188167
[67,  1800] loss: 0.251930526
[67,  2000] loss: 0.244121840
[67,  2200] loss: 0.238279140
[67,  2400] loss: 0.239513800
Final Summary:   loss: 2.437
[68,   200] loss: 0.224969691
[68,   400] loss: 0.233319630
[68,   600] loss: 0.230784714
[68,   800] loss: 0.230860641
[68,  1000] loss: 0.229897502
[68,  1200] loss: 0.231590737
[68,  1400] loss: 0.231270230
[68,  1600] loss: 0.233961838
[68,  1800] loss: 0.232059885
[68,  2000] loss: 0.242267036
[68,  2200] loss: 0.232635874
[68,  2400] loss: 0.232449482
Final Summary:   loss: 2.323
[69,   200] loss: 0.220355664
[69,   400] loss: 0.216827034
[69,   600] loss: 0.215586028
[69,   800] loss: 0.222325957
[69,  1000] loss: 0.225798860
[69,  1200] loss: 0.226629258
[69,  1400] loss: 0.221628501
[69,  1600] loss: 0.213895382
[69,  1800] loss: 0.224577263
[69,  2000] loss: 0.226576127
[69,  2200] loss: 0.219844290
[69,  2400] loss: 0.220103116
Final Summary:   loss: 2.214
[70,   200] loss: 0.212073174
[70,   400] loss: 0.209834641
[70,   600] loss: 0.208115063
[70,   800] loss: 0.212226178
[70,  1000] loss: 0.216076532
[70,  1200] loss: 0.215786184
[70,  1400] loss: 0.210107545
[70,  1600] loss: 0.213751487
[70,  1800] loss: 0.210285503
[70,  2000] loss: 0.214144300
[70,  2200] loss: 0.208413391
[70,  2400] loss: 0.210311515
Final Summary:   loss: 2.120
[71,   200] loss: 0.207544944
[71,   400] loss: 0.199837375
[71,   600] loss: 0.195607176
[71,   800] loss: 0.198288583
[71,  1000] loss: 0.203362821
[71,  1200] loss: 0.200027056
[71,  1400] loss: 0.199654271
[71,  1600] loss: 0.203006241
[71,  1800] loss: 0.203587351
[71,  2000] loss: 0.196516763
[71,  2200] loss: 0.206045665
[71,  2400] loss: 0.213028561
Final Summary:   loss: 2.024
Train Accuracy of the network: 45 %
[72,   200] loss: 0.181242071
[72,   400] loss: 0.190387620
[72,   600] loss: 0.189077425
[72,   800] loss: 0.192013323
[72,  1000] loss: 0.197403684
[72,  1200] loss: 0.196425967
[72,  1400] loss: 0.202039171
[72,  1600] loss: 0.200671538
[72,  1800] loss: 0.199765240
[72,  2000] loss: 0.188637207
[72,  2200] loss: 0.198079447
[72,  2400] loss: 0.196335449
Final Summary:   loss: 1.945
[73,   200] loss: 0.183442955
[73,   400] loss: 0.177860744
[73,   600] loss: 0.180201528
[73,   800] loss: 0.179130412
[73,  1000] loss: 0.178228413
[73,  1200] loss: 0.193668298
[73,  1400] loss: 0.187673503
[73,  1600] loss: 0.184386948
[73,  1800] loss: 0.190092900
[73,  2000] loss: 0.185350687
[73,  2200] loss: 0.192088923
[73,  2400] loss: 0.188205260
Final Summary:   loss: 1.852
[74,   200] loss: 0.167692199
[74,   400] loss: 0.177208878
[74,   600] loss: 0.177947204
[74,   800] loss: 0.176636661
[74,  1000] loss: 0.176794331
[74,  1200] loss: 0.175034062
[74,  1400] loss: 0.178232028
[74,  1600] loss: 0.178274742
[74,  1800] loss: 0.177060378
[74,  2000] loss: 0.181403170
[74,  2200] loss: 0.174851469
[74,  2400] loss: 0.180253901
Final Summary:   loss: 1.770
[75,   200] loss: 0.160471173
[75,   400] loss: 0.165833519
[75,   600] loss: 0.165243722
[75,   800] loss: 0.161055151
[75,  1000] loss: 0.164707279
[75,  1200] loss: 0.166556933
[75,  1400] loss: 0.169468419
[75,  1600] loss: 0.174437029
[75,  1800] loss: 0.172741033
[75,  2000] loss: 0.167865240
[75,  2200] loss: 0.173081234
[75,  2400] loss: 0.173186047
Final Summary:   loss: 1.680
[76,   200] loss: 0.155848717
[76,   400] loss: 0.154934801
[76,   600] loss: 0.154775209
[76,   800] loss: 0.157492914
[76,  1000] loss: 0.160137764
[76,  1200] loss: 0.164933802
[76,  1400] loss: 0.162186991
[76,  1600] loss: 0.161236334
[76,  1800] loss: 0.169318795
[76,  2000] loss: 0.160006619
[76,  2200] loss: 0.163386106
[76,  2400] loss: 0.165788659
Final Summary:   loss: 1.611
Train Accuracy of the network: 50 %
[77,   200] loss: 0.148238156
[77,   400] loss: 0.151752948
[77,   600] loss: 0.149885053
[77,   800] loss: 0.148126570
[77,  1000] loss: 0.151761400
[77,  1200] loss: 0.159493759
[77,  1400] loss: 0.151497125
[77,  1600] loss: 0.152060881
[77,  1800] loss: 0.161337468
[77,  2000] loss: 0.161990861
[77,  2200] loss: 0.157269875
[77,  2400] loss: 0.160665165
Final Summary:   loss: 1.547
[78,   200] loss: 0.138557607
[78,   400] loss: 0.147257376
[78,   600] loss: 0.145555202
[78,   800] loss: 0.146238952
[78,  1000] loss: 0.143615571
[78,  1200] loss: 0.146356426
[78,  1400] loss: 0.150784694
[78,  1600] loss: 0.145610983
[78,  1800] loss: 0.151805185
[78,  2000] loss: 0.151357140
[78,  2200] loss: 0.150325289
[78,  2400] loss: 0.149163711
Final Summary:   loss: 1.472
[79,   200] loss: 0.137029522
[79,   400] loss: 0.132927086
[79,   600] loss: 0.138379812
[79,   800] loss: 0.141784505
[79,  1000] loss: 0.140806628
[79,  1200] loss: 0.142538586
[79,  1400] loss: 0.143629746
[79,  1600] loss: 0.145217476
[79,  1800] loss: 0.147115253
[79,  2000] loss: 0.141411944
[79,  2200] loss: 0.144148113
[79,  2400] loss: 0.145602874
Final Summary:   loss: 1.419
[80,   200] loss: 0.124578544
[80,   400] loss: 0.129471282
[80,   600] loss: 0.136585231
[80,   800] loss: 0.139364585
[80,  1000] loss: 0.135062708
[80,  1200] loss: 0.136756518
[80,  1400] loss: 0.138917795
[80,  1600] loss: 0.140762804
[80,  1800] loss: 0.133475102
[80,  2000] loss: 0.137973816
[80,  2200] loss: 0.139884898
[80,  2400] loss: 0.140683744
Final Summary:   loss: 1.364
[81,   200] loss: 0.124153598
[81,   400] loss: 0.126518785
[81,   600] loss: 0.125895807
[81,   800] loss: 0.125684622
[81,  1000] loss: 0.129636256
[81,  1200] loss: 0.131511787
[81,  1400] loss: 0.124535311
[81,  1600] loss: 0.127899838
[81,  1800] loss: 0.130202075
[81,  2000] loss: 0.134887451
[81,  2200] loss: 0.131073094
[81,  2400] loss: 0.136272057
Final Summary:   loss: 1.292
Train Accuracy of the network: 46 %
[82,   200] loss: 0.120264758
[82,   400] loss: 0.121265320
[82,   600] loss: 0.119013011
[82,   800] loss: 0.123154992
[82,  1000] loss: 0.122662613
[82,  1200] loss: 0.122109241
[82,  1400] loss: 0.128337898
[82,  1600] loss: 0.126015054
[82,  1800] loss: 0.125655165
[82,  2000] loss: 0.120177779
[82,  2200] loss: 0.128736726
[82,  2400] loss: 0.132613692
Final Summary:   loss: 1.242
[83,   200] loss: 0.111443042
[83,   400] loss: 0.113320453
[83,   600] loss: 0.109814988
[83,   800] loss: 0.119435999
[83,  1000] loss: 0.113398234
[83,  1200] loss: 0.115145972
[83,  1400] loss: 0.117997058
[83,  1600] loss: 0.123132332
[83,  1800] loss: 0.126401463
[83,  2000] loss: 0.117735324
[83,  2200] loss: 0.122260231
[83,  2400] loss: 0.126182070
Final Summary:   loss: 1.182
[84,   200] loss: 0.107594988
[84,   400] loss: 0.109709890
[84,   600] loss: 0.115116382
[84,   800] loss: 0.110714700
[84,  1000] loss: 0.111125645
[84,  1200] loss: 0.105783797
[84,  1400] loss: 0.117897271
[84,  1600] loss: 0.113898012
[84,  1800] loss: 0.109994327
[84,  2000] loss: 0.121563579
[84,  2200] loss: 0.116059338
[84,  2400] loss: 0.120274341
Final Summary:   loss: 1.135
[85,   200] loss: 0.104647754
[85,   400] loss: 0.103481096
[85,   600] loss: 0.099797607
[85,   800] loss: 0.104648560
[85,  1000] loss: 0.106974188
[85,  1200] loss: 0.104079057
[85,  1400] loss: 0.108279047
[85,  1600] loss: 0.112968449
[85,  1800] loss: 0.111759027
[85,  2000] loss: 0.116299611
[85,  2200] loss: 0.112937939
[85,  2400] loss: 0.112967076
Final Summary:   loss: 1.084
[86,   200] loss: 0.099051281
[86,   400] loss: 0.096652760
[86,   600] loss: 0.100268980
[86,   800] loss: 0.107730776
[86,  1000] loss: 0.106851799
[86,  1200] loss: 0.104062697
[86,  1400] loss: 0.103705847
[86,  1600] loss: 0.104012629
[86,  1800] loss: 0.106482270
[86,  2000] loss: 0.108591888
[86,  2200] loss: 0.106547585
[86,  2400] loss: 0.108080193
Final Summary:   loss: 1.045
Train Accuracy of the network: 67 %
[87,   200] loss: 0.094467531
[87,   400] loss: 0.091786842
[87,   600] loss: 0.093972192
[87,   800] loss: 0.095465105
[87,  1000] loss: 0.097341954
[87,  1200] loss: 0.100888787
[87,  1400] loss: 0.098139833
[87,  1600] loss: 0.101578586
[87,  1800] loss: 0.100916117
[87,  2000] loss: 0.103388452
[87,  2200] loss: 0.101814377
[87,  2400] loss: 0.106929940
Final Summary:   loss: 0.990
[88,   200] loss: 0.092885320
[88,   400] loss: 0.087675095
[88,   600] loss: 0.087578977
[88,   800] loss: 0.097011946
[88,  1000] loss: 0.093551715
[88,  1200] loss: 0.093766947
[88,  1400] loss: 0.098066921
[88,  1600] loss: 0.098852429
[88,  1800] loss: 0.099482945
[88,  2000] loss: 0.099012864
[88,  2200] loss: 0.096109532
[88,  2400] loss: 0.099639135
Final Summary:   loss: 0.954
[89,   200] loss: 0.085818434
[89,   400] loss: 0.086418059
[89,   600] loss: 0.084303403
[89,   800] loss: 0.092372305
[89,  1000] loss: 0.089748961
[89,  1200] loss: 0.085578852
[89,  1400] loss: 0.095718287
[89,  1600] loss: 0.091984383
[89,  1800] loss: 0.093228593
[89,  2000] loss: 0.098274295
[89,  2200] loss: 0.096214097
[89,  2400] loss: 0.094848554
Final Summary:   loss: 0.915
[90,   200] loss: 0.082743604
[90,   400] loss: 0.084408405
[90,   600] loss: 0.080781398
[90,   800] loss: 0.085507154
[90,  1000] loss: 0.085402306
[90,  1200] loss: 0.085961066
[90,  1400] loss: 0.088956850
[90,  1600] loss: 0.089094237
[90,  1800] loss: 0.088944378
[90,  2000] loss: 0.091363100
[90,  2200] loss: 0.094784485
[90,  2400] loss: 0.090112153
Final Summary:   loss: 0.875
[91,   200] loss: 0.070827319
[91,   400] loss: 0.078609693
[91,   600] loss: 0.080009779
[91,   800] loss: 0.083113462
[91,  1000] loss: 0.082821340
[91,  1200] loss: 0.082629197
[91,  1400] loss: 0.081902144
[91,  1600] loss: 0.087487300
[91,  1800] loss: 0.089981348
[91,  2000] loss: 0.084919041
[91,  2200] loss: 0.086663199
[91,  2400] loss: 0.085078623
Final Summary:   loss: 0.829
Train Accuracy of the network: 66 %
[92,   200] loss: 0.071516149
[92,   400] loss: 0.071326508
[92,   600] loss: 0.073128226
[92,   800] loss: 0.075423593
[92,  1000] loss: 0.078742392
[92,  1200] loss: 0.077537833
[92,  1400] loss: 0.081934528
[92,  1600] loss: 0.084222309
[92,  1800] loss: 0.085473044
[92,  2000] loss: 0.083964097
[92,  2200] loss: 0.082173763
[92,  2400] loss: 0.081122282
Final Summary:   loss: 0.790
[93,   200] loss: 0.069592713
[93,   400] loss: 0.066758565
[93,   600] loss: 0.076703739
[93,   800] loss: 0.070102340
[93,  1000] loss: 0.075133273
[93,  1200] loss: 0.079858318
[93,  1400] loss: 0.075635196
[93,  1600] loss: 0.076695161
[93,  1800] loss: 0.077708891
[93,  2000] loss: 0.077901307
[93,  2200] loss: 0.081176259
[93,  2400] loss: 0.083728266
Final Summary:   loss: 0.760
[94,   200] loss: 0.063845974
[94,   400] loss: 0.065992855
[94,   600] loss: 0.070886246
[94,   800] loss: 0.066717846
[94,  1000] loss: 0.067717984
[94,  1200] loss: 0.074529959
[94,  1400] loss: 0.073023029
[94,  1600] loss: 0.076825934
[94,  1800] loss: 0.073357366
[94,  2000] loss: 0.077037058
[94,  2200] loss: 0.078256735
[94,  2400] loss: 0.074399851
Final Summary:   loss: 0.720
[95,   200] loss: 0.064651389
[95,   400] loss: 0.068343671
[95,   600] loss: 0.064730987
[95,   800] loss: 0.067929412
[95,  1000] loss: 0.066952553
[95,  1200] loss: 0.070248037
[95,  1400] loss: 0.069125109
[95,  1600] loss: 0.068525091
[95,  1800] loss: 0.071541223
[95,  2000] loss: 0.072488131
[95,  2200] loss: 0.077906419
[95,  2400] loss: 0.073599659
Final Summary:   loss: 0.699
[96,   200] loss: 0.060881459
[96,   400] loss: 0.065839251
[96,   600] loss: 0.062505198
[96,   800] loss: 0.058913104
[96,  1000] loss: 0.065814333
[96,  1200] loss: 0.065659739
[96,  1400] loss: 0.064407078
[96,  1600] loss: 0.069468513
[96,  1800] loss: 0.065827869
[96,  2000] loss: 0.071194356
[96,  2200] loss: 0.071149322
[96,  2400] loss: 0.068367272
Final Summary:   loss: 0.660
Train Accuracy of the network: 63 %
[97,   200] loss: 0.058429827
[97,   400] loss: 0.057242368
[97,   600] loss: 0.056485247
[97,   800] loss: 0.059596898
[97,  1000] loss: 0.064875588
[97,  1200] loss: 0.061111523
[97,  1400] loss: 0.064917723
[97,  1600] loss: 0.062216463
[97,  1800] loss: 0.066691843
[97,  2000] loss: 0.066340532
[97,  2200] loss: 0.067299066
[97,  2400] loss: 0.062836359
Final Summary:   loss: 0.625
[98,   200] loss: 0.057574397
[98,   400] loss: 0.052990435
[98,   600] loss: 0.058983865
[98,   800] loss: 0.057016831
[98,  1000] loss: 0.057021236
[98,  1200] loss: 0.061768183
[98,  1400] loss: 0.059681467
[98,  1600] loss: 0.061434146
[98,  1800] loss: 0.059025203
[98,  2000] loss: 0.063203990
[98,  2200] loss: 0.059967988
[98,  2400] loss: 0.061685602
Final Summary:   loss: 0.594
[99,   200] loss: 0.050915126
[99,   400] loss: 0.053271292
[99,   600] loss: 0.054142875
[99,   800] loss: 0.051346015
[99,  1000] loss: 0.055976490
[99,  1200] loss: 0.056908728
[99,  1400] loss: 0.056585506
[99,  1600] loss: 0.062341509
[99,  1800] loss: 0.059666550
[99,  2000] loss: 0.059566553
[99,  2200] loss: 0.059410779
[99,  2400] loss: 0.063579861
Final Summary:   loss: 0.571
[100,   200] loss: 0.050165410
[100,   400] loss: 0.051447374
[100,   600] loss: 0.051153947
[100,   800] loss: 0.055026587
[100,  1000] loss: 0.053883270
[100,  1200] loss: 0.055076853
[100,  1400] loss: 0.054582873
[100,  1600] loss: 0.059639689
[100,  1800] loss: 0.054407652
[100,  2000] loss: 0.056225864
[100,  2200] loss: 0.056707301
[100,  2400] loss: 0.059103948
Final Summary:   loss: 0.549
[101,   200] loss: 0.045307975
[101,   400] loss: 0.046839906
[101,   600] loss: 0.047046425
[101,   800] loss: 0.051309357
[101,  1000] loss: 0.052321946
[101,  1200] loss: 0.049733572
[101,  1400] loss: 0.054339157
[101,  1600] loss: 0.052549429
[101,  1800] loss: 0.055683476
[101,  2000] loss: 0.057459713
[101,  2200] loss: 0.054739217
[101,  2400] loss: 0.057407838
Final Summary:   loss: 0.521
Train Accuracy of the network: 79 %
[102,   200] loss: 0.044537646
[102,   400] loss: 0.045541677
[102,   600] loss: 0.047838404
[102,   800] loss: 0.047350918
[102,  1000] loss: 0.045389911
[102,  1200] loss: 0.047950789
[102,  1400] loss: 0.051753249
[102,  1600] loss: 0.047188258
[102,  1800] loss: 0.051130640
[102,  2000] loss: 0.054083051
[102,  2200] loss: 0.052492075
[102,  2400] loss: 0.059613755
Final Summary:   loss: 0.497
[103,   200] loss: 0.046211514
[103,   400] loss: 0.041467429
[103,   600] loss: 0.046287179
[103,   800] loss: 0.045894841
[103,  1000] loss: 0.044319799
[103,  1200] loss: 0.045795144
[103,  1400] loss: 0.044131093
[103,  1600] loss: 0.046943300
[103,  1800] loss: 0.049779913
[103,  2000] loss: 0.051649845
[103,  2200] loss: 0.052603616
[103,  2400] loss: 0.054558276
Final Summary:   loss: 0.476
[104,   200] loss: 0.042548843
[104,   400] loss: 0.043451633
[104,   600] loss: 0.040663741
[104,   800] loss: 0.042343644
[104,  1000] loss: 0.043304600
[104,  1200] loss: 0.044765292
[104,  1400] loss: 0.047717212
[104,  1600] loss: 0.048035117
[104,  1800] loss: 0.046895869
[104,  2000] loss: 0.048111492
[104,  2200] loss: 0.049133932
[104,  2400] loss: 0.052643541
Final Summary:   loss: 0.460
[105,   200] loss: 0.038835150
[105,   400] loss: 0.038349373
[105,   600] loss: 0.040092893
[105,   800] loss: 0.041114574
[105,  1000] loss: 0.041250770
[105,  1200] loss: 0.039919485
[105,  1400] loss: 0.045752400
[105,  1600] loss: 0.045651314
[105,  1800] loss: 0.044502119
[105,  2000] loss: 0.046963153
[105,  2200] loss: 0.046934637
[105,  2400] loss: 0.049024979
Final Summary:   loss: 0.433
[106,   200] loss: 0.040492902
[106,   400] loss: 0.034263747
[106,   600] loss: 0.036397661
[106,   800] loss: 0.041589374
[106,  1000] loss: 0.043195164
[106,  1200] loss: 0.040098650
[106,  1400] loss: 0.042218160
[106,  1600] loss: 0.042658935
[106,  1800] loss: 0.040703799
[106,  2000] loss: 0.042879372
[106,  2200] loss: 0.043236262
[106,  2400] loss: 0.047615111
Final Summary:   loss: 0.414
Train Accuracy of the network: 88 %
[107,   200] loss: 0.031961944
[107,   400] loss: 0.032875106
[107,   600] loss: 0.035913276
[107,   800] loss: 0.038724396
[107,  1000] loss: 0.037713328
[107,  1200] loss: 0.039387366
[107,  1400] loss: 0.041749693
[107,  1600] loss: 0.044269994
[107,  1800] loss: 0.040868750
[107,  2000] loss: 0.041259340
[107,  2200] loss: 0.043680499
[107,  2400] loss: 0.045286219
Final Summary:   loss: 0.397
[108,   200] loss: 0.042015528
[108,   400] loss: 0.034760521
[108,   600] loss: 0.035080397
[108,   800] loss: 0.036133483
[108,  1000] loss: 0.036090316
[108,  1200] loss: 0.036873657
[108,  1400] loss: 0.037053183
[108,  1600] loss: 0.039876255
[108,  1800] loss: 0.039298017
[108,  2000] loss: 0.038809519
[108,  2200] loss: 0.038557463
[108,  2400] loss: 0.041763683
Final Summary:   loss: 0.381
[109,   200] loss: 0.034504366
[109,   400] loss: 0.032411834
[109,   600] loss: 0.033266691
[109,   800] loss: 0.032592344
[109,  1000] loss: 0.033387377
[109,  1200] loss: 0.030294510
[109,  1400] loss: 0.034757555
[109,  1600] loss: 0.037808068
[109,  1800] loss: 0.035538726
[109,  2000] loss: 0.037901863
[109,  2200] loss: 0.039250051
[109,  2400] loss: 0.041858301
Final Summary:   loss: 0.356
[110,   200] loss: 0.032379648
[110,   400] loss: 0.033025549
[110,   600] loss: 0.032190724
[110,   800] loss: 0.034543353
[110,  1000] loss: 0.036193274
[110,  1200] loss: 0.033972765
[110,  1400] loss: 0.031714069
[110,  1600] loss: 0.035255196
[110,  1800] loss: 0.034619455
[110,  2000] loss: 0.039137688
[110,  2200] loss: 0.038717341
[110,  2400] loss: 0.037993931
Final Summary:   loss: 0.351
[111,   200] loss: 0.038773016
[111,   400] loss: 0.030071807
[111,   600] loss: 0.033796472
[111,   800] loss: 0.029992185
[111,  1000] loss: 0.034177069
[111,  1200] loss: 0.034429031
[111,  1400] loss: 0.033729161
[111,  1600] loss: 0.035563579
[111,  1800] loss: 0.031927917
[111,  2000] loss: 0.035033618
[111,  2200] loss: 0.039937243
[111,  2400] loss: 0.035614302
Final Summary:   loss: 0.345
Train Accuracy of the network: 65 %
[112,   200] loss: 0.032283967
[112,   400] loss: 0.028712505
[112,   600] loss: 0.031126302
[112,   800] loss: 0.033088836
[112,  1000] loss: 0.031461433
[112,  1200] loss: 0.032389703
[112,  1400] loss: 0.031313878
[112,  1600] loss: 0.031529001
[112,  1800] loss: 0.036894187
[112,  2000] loss: 0.032450541
[112,  2200] loss: 0.035089166
[112,  2400] loss: 0.032937193
Final Summary:   loss: 0.327
[113,   200] loss: 0.032077024
[113,   400] loss: 0.032202445
[113,   600] loss: 0.028886045
[113,   800] loss: 0.029554540
[113,  1000] loss: 0.031700838
[113,  1200] loss: 0.029754991
[113,  1400] loss: 0.031740321
[113,  1600] loss: 0.032290150
[113,  1800] loss: 0.031887748
[113,  2000] loss: 0.033193074
[113,  2200] loss: 0.032412066
[113,  2400] loss: 0.033420943
Final Summary:   loss: 0.317
[114,   200] loss: 0.028986572
[114,   400] loss: 0.025565342
[114,   600] loss: 0.025600460
[114,   800] loss: 0.026911396
[114,  1000] loss: 0.025464884
[114,  1200] loss: 0.028577170
[114,  1400] loss: 0.029198495
[114,  1600] loss: 0.031727205
[114,  1800] loss: 0.030046774
[114,  2000] loss: 0.029297171
[114,  2200] loss: 0.031652812
[114,  2400] loss: 0.033701611
Final Summary:   loss: 0.289
[115,   200] loss: 0.026179153
[115,   400] loss: 0.022814565
[115,   600] loss: 0.026421240
[115,   800] loss: 0.025164673
[115,  1000] loss: 0.026900740
[115,  1200] loss: 0.026116303
[115,  1400] loss: 0.028096897
[115,  1600] loss: 0.029493443
[115,  1800] loss: 0.029266653
[115,  2000] loss: 0.031306513
[115,  2200] loss: 0.031224912
[115,  2400] loss: 0.033336421
Final Summary:   loss: 0.281
[116,   200] loss: 0.028600835
[116,   400] loss: 0.027567182
[116,   600] loss: 0.022630187
[116,   800] loss: 0.024526826
[116,  1000] loss: 0.028016400
[116,  1200] loss: 0.027547762
[116,  1400] loss: 0.030698871
[116,  1600] loss: 0.027873730
[116,  1800] loss: 0.029994195
[116,  2000] loss: 0.028060129
[116,  2200] loss: 0.030987626
[116,  2400] loss: 0.032066520
Final Summary:   loss: 0.283
Train Accuracy of the network: 73 %
[117,   200] loss: 0.028453258
[117,   400] loss: 0.024822333
[117,   600] loss: 0.023756951
[117,   800] loss: 0.025626755
[117,  1000] loss: 0.024223700
[117,  1200] loss: 0.025000654
[117,  1400] loss: 0.024849781
[117,  1600] loss: 0.026739163
[117,  1800] loss: 0.027286635
[117,  2000] loss: 0.029074809
[117,  2200] loss: 0.028511172
[117,  2400] loss: 0.025994656
Final Summary:   loss: 0.264
[118,   200] loss: 0.028514517
[118,   400] loss: 0.022652782
[118,   600] loss: 0.026466744
[118,   800] loss: 0.024804776
[118,  1000] loss: 0.022486036
[118,  1200] loss: 0.025975055
[118,  1400] loss: 0.026765146
[118,  1600] loss: 0.025603279
[118,  1800] loss: 0.024670488
[118,  2000] loss: 0.028505811
[118,  2200] loss: 0.028682509
[118,  2400] loss: 0.029986142
Final Summary:   loss: 0.265
[119,   200] loss: 0.026532039
[119,   400] loss: 0.023902649
[119,   600] loss: 0.021340475
[119,   800] loss: 0.023346531
[119,  1000] loss: 0.026691060
[119,  1200] loss: 0.024695659
[119,  1400] loss: 0.026177377
[119,  1600] loss: 0.023039412
[119,  1800] loss: 0.024166228
[119,  2000] loss: 0.024236996
[119,  2200] loss: 0.028959329
[119,  2400] loss: 0.027237045
Final Summary:   loss: 0.251
[120,   200] loss: 0.022738251
[120,   400] loss: 0.021925399
[120,   600] loss: 0.022368297
[120,   800] loss: 0.022095170
[120,  1000] loss: 0.023060632
[120,  1200] loss: 0.024090856
[120,  1400] loss: 0.023087401
[120,  1600] loss: 0.024650616
[120,  1800] loss: 0.024825817
[120,  2000] loss: 0.026118194
[120,  2200] loss: 0.026691644
[120,  2400] loss: 0.027220396
Final Summary:   loss: 0.242
[121,   200] loss: 0.023025360
[121,   400] loss: 0.018555862
[121,   600] loss: 0.021727543
[121,   800] loss: 0.021322770
[121,  1000] loss: 0.019609953
[121,  1200] loss: 0.020047881
[121,  1400] loss: 0.022830752
[121,  1600] loss: 0.025502853
[121,  1800] loss: 0.020804816
[121,  2000] loss: 0.023013764
[121,  2200] loss: 0.022136819
[121,  2400] loss: 0.024319666
Final Summary:   loss: 0.220
Train Accuracy of the network: 82 %
[122,   200] loss: 0.022653211
[122,   400] loss: 0.019144330
[122,   600] loss: 0.018581614
[122,   800] loss: 0.020183325
[122,  1000] loss: 0.023015598
[122,  1200] loss: 0.020668829
[122,  1400] loss: 0.019289410
[122,  1600] loss: 0.019261874
[122,  1800] loss: 0.024565109
[122,  2000] loss: 0.021715612
[122,  2200] loss: 0.023217513
[122,  2400] loss: 0.021745576
Final Summary:   loss: 0.212
[123,   200] loss: 0.021207189
[123,   400] loss: 0.019604270
[123,   600] loss: 0.020515668
[123,   800] loss: 0.022339961
[123,  1000] loss: 0.021772205
[123,  1200] loss: 0.022381541
[123,  1400] loss: 0.021077121
[123,  1600] loss: 0.021043797
[123,  1800] loss: 0.019872291
[123,  2000] loss: 0.021765601
[123,  2200] loss: 0.021635286
[123,  2400] loss: 0.021595728
Final Summary:   loss: 0.214
[124,   200] loss: 0.021131514
[124,   400] loss: 0.017145918
[124,   600] loss: 0.016015571
[124,   800] loss: 0.016731248
[124,  1000] loss: 0.019562935
[124,  1200] loss: 0.017479034
[124,  1400] loss: 0.021154173
[124,  1600] loss: 0.018503380
[124,  1800] loss: 0.022292653
[124,  2000] loss: 0.022503623
[124,  2200] loss: 0.022632671
[124,  2400] loss: 0.020197346
Final Summary:   loss: 0.197
[125,   200] loss: 0.019305497
[125,   400] loss: 0.017428698
[125,   600] loss: 0.019537350
[125,   800] loss: 0.017143066
[125,  1000] loss: 0.022579792
[125,  1200] loss: 0.020315599
[125,  1400] loss: 0.023070636
[125,  1600] loss: 0.025208565
[125,  1800] loss: 0.022981543
[125,  2000] loss: 0.022643945
[125,  2200] loss: 0.020075837
[125,  2400] loss: 0.021568137
Final Summary:   loss: 0.212
[126,   200] loss: 0.022165680
[126,   400] loss: 0.018296492
[126,   600] loss: 0.019433211
[126,   800] loss: 0.017938656
[126,  1000] loss: 0.015239149
[126,  1200] loss: 0.015387463
[126,  1400] loss: 0.019309586
[126,  1600] loss: 0.015984851
[126,  1800] loss: 0.020020820
[126,  2000] loss: 0.018231945
[126,  2200] loss: 0.020437013
[126,  2400] loss: 0.020208685
Final Summary:   loss: 0.186
Train Accuracy of the network: 81 %
[127,   200] loss: 0.020652247
[127,   400] loss: 0.016249920
[127,   600] loss: 0.016614584
[127,   800] loss: 0.017019811
[127,  1000] loss: 0.017402165
[127,  1200] loss: 0.017945340
[127,  1400] loss: 0.017981124
[127,  1600] loss: 0.017590002
[127,  1800] loss: 0.018833262
[127,  2000] loss: 0.019764948
[127,  2200] loss: 0.023287390
[127,  2400] loss: 0.018457988
Final Summary:   loss: 0.186
[128,   200] loss: 0.019787233
[128,   400] loss: 0.015601612
[128,   600] loss: 0.016360141
[128,   800] loss: 0.016353029
[128,  1000] loss: 0.016896585
[128,  1200] loss: 0.017765688
[128,  1400] loss: 0.017628082
[128,  1600] loss: 0.019426691
[128,  1800] loss: 0.017238723
[128,  2000] loss: 0.018952866
[128,  2200] loss: 0.017776738
[128,  2400] loss: 0.018787042
Final Summary:   loss: 0.178
[129,   200] loss: 0.015729844
[129,   400] loss: 0.013674839
[129,   600] loss: 0.013744719
[129,   800] loss: 0.016730539
[129,  1000] loss: 0.016999159
[129,  1200] loss: 0.016733529
[129,  1400] loss: 0.017423853
[129,  1600] loss: 0.019896073
[129,  1800] loss: 0.017770493
[129,  2000] loss: 0.020279015
[129,  2200] loss: 0.018755817
[129,  2400] loss: 0.018267336
Final Summary:   loss: 0.173
[130,   200] loss: 0.017514853
[130,   400] loss: 0.018121483
[130,   600] loss: 0.018840514
[130,   800] loss: 0.015159846
[130,  1000] loss: 0.016441486
[130,  1200] loss: 0.020033655
[130,  1400] loss: 0.017078257
[130,  1600] loss: 0.017425830
[130,  1800] loss: 0.018687690
[130,  2000] loss: 0.017895481
[130,  2200] loss: 0.015758494
[130,  2400] loss: 0.017589462
Final Summary:   loss: 0.177
[131,   200] loss: 0.019161312
[131,   400] loss: 0.012913265
[131,   600] loss: 0.016648387
[131,   800] loss: 0.015541749
[131,  1000] loss: 0.015875279
[131,  1200] loss: 0.018293598
[131,  1400] loss: 0.016033720
[131,  1600] loss: 0.017271645
[131,  1800] loss: 0.017986618
[131,  2000] loss: 0.018799729
[131,  2200] loss: 0.017573533
[131,  2400] loss: 0.017349051
Final Summary:   loss: 0.170
Train Accuracy of the network: 95 %
[132,   200] loss: 0.016445086
[132,   400] loss: 0.015436104
[132,   600] loss: 0.016099076
[132,   800] loss: 0.018133584
[132,  1000] loss: 0.014835076
[132,  1200] loss: 0.016962611
[132,  1400] loss: 0.017178654
[132,  1600] loss: 0.021269112
[132,  1800] loss: 0.018950024
[132,  2000] loss: 0.017579100
[132,  2200] loss: 0.017530444
[132,  2400] loss: 0.016603771
Final Summary:   loss: 0.173
[133,   200] loss: 0.015125083
[133,   400] loss: 0.015386897
[133,   600] loss: 0.012316384
[133,   800] loss: 0.014088131
[133,  1000] loss: 0.014589597
[133,  1200] loss: 0.012251831
[133,  1400] loss: 0.015897571
[133,  1600] loss: 0.016477554
[133,  1800] loss: 0.016209947
[133,  2000] loss: 0.015157148
[133,  2200] loss: 0.015628761
[133,  2400] loss: 0.013261243
Final Summary:   loss: 0.149
[134,   200] loss: 0.020552045
[134,   400] loss: 0.016241408
[134,   600] loss: 0.016064211
[134,   800] loss: 0.015677953
[134,  1000] loss: 0.014445546
[134,  1200] loss: 0.014739340
[134,  1400] loss: 0.015714844
[134,  1600] loss: 0.015818875
[134,  1800] loss: 0.015541815
[134,  2000] loss: 0.014051504
[134,  2200] loss: 0.015241904
[134,  2400] loss: 0.013614844
Final Summary:   loss: 0.157
[135,   200] loss: 0.016585704
[135,   400] loss: 0.013011903
[135,   600] loss: 0.011560507
[135,   800] loss: 0.013643498
[135,  1000] loss: 0.012504950
[135,  1200] loss: 0.013411953
[135,  1400] loss: 0.013080508
[135,  1600] loss: 0.014553720
[135,  1800] loss: 0.016121210
[135,  2000] loss: 0.014604295
[135,  2200] loss: 0.017459768
[135,  2400] loss: 0.016460510
Final Summary:   loss: 0.145
[136,   200] loss: 0.015642727
[136,   400] loss: 0.014940760
[136,   600] loss: 0.013123474
[136,   800] loss: 0.016587274
[136,  1000] loss: 0.013243197
[136,  1200] loss: 0.015682927
[136,  1400] loss: 0.012751137
[136,  1600] loss: 0.014691854
[136,  1800] loss: 0.015181302
[136,  2000] loss: 0.016522617
[136,  2200] loss: 0.015018218
[136,  2400] loss: 0.014924150
Final Summary:   loss: 0.149
Train Accuracy of the network: 93 %
[137,   200] loss: 0.012045497
[137,   400] loss: 0.009842183
[137,   600] loss: 0.012039117
[137,   800] loss: 0.012403258
[137,  1000] loss: 0.013851711
[137,  1200] loss: 0.013518275
[137,  1400] loss: 0.013178758
[137,  1600] loss: 0.014733278
[137,  1800] loss: 0.013026202
[137,  2000] loss: 0.015402610
[137,  2200] loss: 0.015729967
[137,  2400] loss: 0.017907080
Final Summary:   loss: 0.138
[138,   200] loss: 0.016189636
[138,   400] loss: 0.014625364
[138,   600] loss: 0.012206758
[138,   800] loss: 0.012558299
[138,  1000] loss: 0.013870514
[138,  1200] loss: 0.015254378
[138,  1400] loss: 0.012046717
[138,  1600] loss: 0.012151301
[138,  1800] loss: 0.014088504
[138,  2000] loss: 0.016562367
[138,  2200] loss: 0.014882143
[138,  2400] loss: 0.013737012
Final Summary:   loss: 0.141
[139,   200] loss: 0.013473528
[139,   400] loss: 0.010941031
[139,   600] loss: 0.012400194
[139,   800] loss: 0.016128136
[139,  1000] loss: 0.015043688
[139,  1200] loss: 0.013844748
[139,  1400] loss: 0.012739969
[139,  1600] loss: 0.013233944
[139,  1800] loss: 0.010082688
[139,  2000] loss: 0.014120992
[139,  2200] loss: 0.014121560
[139,  2400] loss: 0.014041289
Final Summary:   loss: 0.134
[140,   200] loss: 0.014011750
[140,   400] loss: 0.011481511
[140,   600] loss: 0.013764631
[140,   800] loss: 0.013857741
[140,  1000] loss: 0.013106790
[140,  1200] loss: 0.012065069
[140,  1400] loss: 0.014083108
[140,  1600] loss: 0.012085142
[140,  1800] loss: 0.013475412
[140,  2000] loss: 0.015516608
[140,  2200] loss: 0.013220976
[140,  2400] loss: 0.014808584
Final Summary:   loss: 0.135
[141,   200] loss: 0.012843180
[141,   400] loss: 0.015738391
[141,   600] loss: 0.013408455
[141,   800] loss: 0.013634585
[141,  1000] loss: 0.012777375
[141,  1200] loss: 0.013922174
[141,  1400] loss: 0.011910259
[141,  1600] loss: 0.012659146
[141,  1800] loss: 0.012730242
[141,  2000] loss: 0.013129047
[141,  2200] loss: 0.012587101
[141,  2400] loss: 0.013595492
Final Summary:   loss: 0.133
Train Accuracy of the network: 80 %
[142,   200] loss: 0.015988110
[142,   400] loss: 0.012141577
[142,   600] loss: 0.010192593
[142,   800] loss: 0.011651011
[142,  1000] loss: 0.012324164
[142,  1200] loss: 0.011489875
[142,  1400] loss: 0.011337865
[142,  1600] loss: 0.012967041
[142,  1800] loss: 0.011485018
[142,  2000] loss: 0.013002099
[142,  2200] loss: 0.012978418
[142,  2400] loss: 0.013743362
Final Summary:   loss: 0.125
[143,   200] loss: 0.014169540
[143,   400] loss: 0.014493773
[143,   600] loss: 0.012471332
[143,   800] loss: 0.011749446
[143,  1000] loss: 0.012522521
[143,  1200] loss: 0.013898228
[143,  1400] loss: 0.013109461
[143,  1600] loss: 0.011903123
[143,  1800] loss: 0.012034961
[143,  2000] loss: 0.014076455
[143,  2200] loss: 0.012726626
[143,  2400] loss: 0.012214518
Final Summary:   loss: 0.131
[144,   200] loss: 0.015132057
[144,   400] loss: 0.011741333
[144,   600] loss: 0.011224520
[144,   800] loss: 0.010922375
[144,  1000] loss: 0.009776274
[144,  1200] loss: 0.009716020
[144,  1400] loss: 0.012622512
[144,  1600] loss: 0.010085987
[144,  1800] loss: 0.011833564
[144,  2000] loss: 0.011637779
[144,  2200] loss: 0.011896915
[144,  2400] loss: 0.010882127
Final Summary:   loss: 0.116
[145,   200] loss: 0.014999135
[145,   400] loss: 0.010597413
[145,   600] loss: 0.009495638
[145,   800] loss: 0.011317206
[145,  1000] loss: 0.010795136
[145,  1200] loss: 0.011750028
[145,  1400] loss: 0.012539627
[145,  1600] loss: 0.012471247
[145,  1800] loss: 0.011183462
[145,  2000] loss: 0.011851271
[145,  2200] loss: 0.013326845
[145,  2400] loss: 0.012033795
Final Summary:   loss: 0.119
[146,   200] loss: 0.010443871
[146,   400] loss: 0.009480576
[146,   600] loss: 0.012533662
[146,   800] loss: 0.011252984
[146,  1000] loss: 0.011653574
[146,  1200] loss: 0.011332137
[146,  1400] loss: 0.009890995
[146,  1600] loss: 0.010959489
[146,  1800] loss: 0.011399051
[146,  2000] loss: 0.012438558
[146,  2200] loss: 0.010356871
[146,  2400] loss: 0.010985323
Final Summary:   loss: 0.111
Train Accuracy of the network: 95 %
[147,   200] loss: 0.011178199
[147,   400] loss: 0.008057840
[147,   600] loss: 0.010037029
[147,   800] loss: 0.008094673
[147,  1000] loss: 0.009611582
[147,  1200] loss: 0.008039998
[147,  1400] loss: 0.009254414
[147,  1600] loss: 0.009157118
[147,  1800] loss: 0.009527497
[147,  2000] loss: 0.010398736
[147,  2200] loss: 0.010343628
[147,  2400] loss: 0.012850741
Final Summary:   loss: 0.099
[148,   200] loss: 0.014532049
[148,   400] loss: 0.009340358
[148,   600] loss: 0.010649184
[148,   800] loss: 0.009358295
[148,  1000] loss: 0.010941143
[148,  1200] loss: 0.011391121
[148,  1400] loss: 0.011295964
[148,  1600] loss: 0.010639161
[148,  1800] loss: 0.011977651
[148,  2000] loss: 0.012640033
[148,  2200] loss: 0.011019080
[148,  2400] loss: 0.011548541
Final Summary:   loss: 0.114
[149,   200] loss: 0.012201966
[149,   400] loss: 0.009728610
[149,   600] loss: 0.010296073
[149,   800] loss: 0.008929091
[149,  1000] loss: 0.011951329
[149,  1200] loss: 0.014285674
[149,  1400] loss: 0.012613647
[149,  1600] loss: 0.011599960
[149,  1800] loss: 0.011878916
[149,  2000] loss: 0.012211500
[149,  2200] loss: 0.011451745
[149,  2400] loss: 0.009840302
Final Summary:   loss: 0.114
[150,   200] loss: 0.008062178
[150,   400] loss: 0.007959446
[150,   600] loss: 0.009619117
[150,   800] loss: 0.008077298
[150,  1000] loss: 0.007171585
[150,  1200] loss: 0.009296535
[150,  1400] loss: 0.006296815
[150,  1600] loss: 0.009340415
[150,  1800] loss: 0.010300728
[150,  2000] loss: 0.007997376
[150,  2200] loss: 0.009890358
[150,  2400] loss: 0.012451815
Final Summary:   loss: 0.089
[151,   200] loss: 0.009055785
[151,   400] loss: 0.009126900
[151,   600] loss: 0.010222907
[151,   800] loss: 0.010535565
[151,  1000] loss: 0.011286372
[151,  1200] loss: 0.009500441
[151,  1400] loss: 0.008328023
[151,  1600] loss: 0.008650385
[151,  1800] loss: 0.008240484
[151,  2000] loss: 0.009607293
[151,  2200] loss: 0.008585496
[151,  2400] loss: 0.011523463
Final Summary:   loss: 0.096
Train Accuracy of the network: 83 %
[152,   200] loss: 0.013359128
[152,   400] loss: 0.008421299
[152,   600] loss: 0.008407842
[152,   800] loss: 0.009525306
[152,  1000] loss: 0.010165114
[152,  1200] loss: 0.010011918
[152,  1400] loss: 0.009929719
[152,  1600] loss: 0.010046364
[152,  1800] loss: 0.010918582
[152,  2000] loss: 0.012151971
[152,  2200] loss: 0.009656867
[152,  2400] loss: 0.010634898
Final Summary:   loss: 0.103
[153,   200] loss: 0.007873338
[153,   400] loss: 0.007377957
[153,   600] loss: 0.007913041
[153,   800] loss: 0.009343472
[153,  1000] loss: 0.008962607
[153,  1200] loss: 0.009846734
[153,  1400] loss: 0.008706604
[153,  1600] loss: 0.010148796
[153,  1800] loss: 0.010114136
[153,  2000] loss: 0.010788162
[153,  2200] loss: 0.010938434
[153,  2400] loss: 0.009776940
Final Summary:   loss: 0.094
[154,   200] loss: 0.010499083
[154,   400] loss: 0.008543136
[154,   600] loss: 0.008957422
[154,   800] loss: 0.007890983
[154,  1000] loss: 0.009286735
[154,  1200] loss: 0.009795144
[154,  1400] loss: 0.009399831
[154,  1600] loss: 0.011161305
[154,  1800] loss: 0.010234140
[154,  2000] loss: 0.008726610
[154,  2200] loss: 0.009661055
[154,  2400] loss: 0.010443366
Final Summary:   loss: 0.097
[155,   200] loss: 0.011594502
[155,   400] loss: 0.008215206
[155,   600] loss: 0.008517930
[155,   800] loss: 0.006971110
[155,  1000] loss: 0.008901165
[155,  1200] loss: 0.007480373
[155,  1400] loss: 0.007567767
[155,  1600] loss: 0.008533417
[155,  1800] loss: 0.009355608
[155,  2000] loss: 0.006882353
[155,  2200] loss: 0.009326320
[155,  2400] loss: 0.009064556
Final Summary:   loss: 0.085
[156,   200] loss: 0.007307986
[156,   400] loss: 0.008793057
[156,   600] loss: 0.008286088
[156,   800] loss: 0.008005035
[156,  1000] loss: 0.008805524
[156,  1200] loss: 0.009934700
[156,  1400] loss: 0.010443339
[156,  1600] loss: 0.008161147
[156,  1800] loss: 0.008916790
[156,  2000] loss: 0.008788721
[156,  2200] loss: 0.008668286
[156,  2400] loss: 0.010446708
Final Summary:   loss: 0.090
Train Accuracy of the network: 66 %
[157,   200] loss: 0.012298503
[157,   400] loss: 0.008706201
[157,   600] loss: 0.008896645
[157,   800] loss: 0.011393122
[157,  1000] loss: 0.008990914
[157,  1200] loss: 0.009337744
[157,  1400] loss: 0.007077226
[157,  1600] loss: 0.008970055
[157,  1800] loss: 0.007687954
[157,  2000] loss: 0.008755575
[157,  2200] loss: 0.010438131
[157,  2400] loss: 0.008930062
Final Summary:   loss: 0.095
[158,   200] loss: 0.015624056
[158,   400] loss: 0.008234170
[158,   600] loss: 0.007001599
[158,   800] loss: 0.009266265
[158,  1000] loss: 0.008624578
[158,  1200] loss: 0.009123765
[158,  1400] loss: 0.010297190
[158,  1600] loss: 0.009016796
[158,  1800] loss: 0.009838844
[158,  2000] loss: 0.011629900
[158,  2200] loss: 0.009028256
[158,  2400] loss: 0.008958039
Final Summary:   loss: 0.098
[159,   200] loss: 0.009863297
[159,   400] loss: 0.007153514
[159,   600] loss: 0.005592746
[159,   800] loss: 0.007679813
[159,  1000] loss: 0.007960750
[159,  1200] loss: 0.007964650
[159,  1400] loss: 0.008599509
[159,  1600] loss: 0.007605841
[159,  1800] loss: 0.008707592
[159,  2000] loss: 0.007894364
[159,  2200] loss: 0.009143602
[159,  2400] loss: 0.009276594
Final Summary:   loss: 0.082
[160,   200] loss: 0.011276785
[160,   400] loss: 0.008925759
[160,   600] loss: 0.007858432
[160,   800] loss: 0.009541994
[160,  1000] loss: 0.007036158
[160,  1200] loss: 0.008562846
[160,  1400] loss: 0.007731509
[160,  1600] loss: 0.009143321
[160,  1800] loss: 0.008350932
[160,  2000] loss: 0.008200437
[160,  2200] loss: 0.007972534
[160,  2400] loss: 0.007411739
Final Summary:   loss: 0.086
[161,   200] loss: 0.008027733
[161,   400] loss: 0.006679635
[161,   600] loss: 0.007382893
[161,   800] loss: 0.008974623
[161,  1000] loss: 0.009613655
[161,  1200] loss: 0.009242040
[161,  1400] loss: 0.010026997
[161,  1600] loss: 0.006899534
[161,  1800] loss: 0.008529395
[161,  2000] loss: 0.008729762
[161,  2200] loss: 0.007458831
[161,  2400] loss: 0.008103939
Final Summary:   loss: 0.084
Train Accuracy of the network: 75 %
[162,   200] loss: 0.009718172
[162,   400] loss: 0.007558710
[162,   600] loss: 0.007327605
[162,   800] loss: 0.009458908
[162,  1000] loss: 0.009679606
[162,  1200] loss: 0.008458013
[162,  1400] loss: 0.007027461
[162,  1600] loss: 0.008637803
[162,  1800] loss: 0.007365799
[162,  2000] loss: 0.009224305
[162,  2200] loss: 0.009057111
[162,  2400] loss: 0.009574185
Final Summary:   loss: 0.087
[163,   200] loss: 0.012111636
[163,   400] loss: 0.008950631
[163,   600] loss: 0.007956005
[163,   800] loss: 0.007839104
[163,  1000] loss: 0.007161622
[163,  1200] loss: 0.006944507
[163,  1400] loss: 0.007371152
[163,  1600] loss: 0.006109876
[163,  1800] loss: 0.007649000
[163,  2000] loss: 0.006166151
[163,  2200] loss: 0.007336466
[163,  2400] loss: 0.008496021
Final Summary:   loss: 0.080
[164,   200] loss: 0.012672931
[164,   400] loss: 0.007341643
[164,   600] loss: 0.008653648
[164,   800] loss: 0.008585866
[164,  1000] loss: 0.008498567
[164,  1200] loss: 0.008606363
[164,  1400] loss: 0.008146833
[164,  1600] loss: 0.008484272
[164,  1800] loss: 0.008560028
[164,  2000] loss: 0.007406351
[164,  2200] loss: 0.007356247
[164,  2400] loss: 0.007755452
Final Summary:   loss: 0.085
[165,   200] loss: 0.007447161
[165,   400] loss: 0.006614191
[165,   600] loss: 0.006849119
[165,   800] loss: 0.007916772
[165,  1000] loss: 0.007358123
[165,  1200] loss: 0.009096329
[165,  1400] loss: 0.009605099
[165,  1600] loss: 0.008665361
[165,  1800] loss: 0.007612419
[165,  2000] loss: 0.009335447
[165,  2200] loss: 0.006508880
[165,  2400] loss: 0.007658376
Final Summary:   loss: 0.080
[166,   200] loss: 0.009960944
[166,   400] loss: 0.008413334
[166,   600] loss: 0.006450946
[166,   800] loss: 0.007210584
[166,  1000] loss: 0.007547341
[166,  1200] loss: 0.009096795
[166,  1400] loss: 0.009937008
[166,  1600] loss: 0.007909166
[166,  1800] loss: 0.007207326
[166,  2000] loss: 0.006314368
[166,  2200] loss: 0.008316689
[166,  2400] loss: 0.007378760
Final Summary:   loss: 0.082
Train Accuracy of the network: 80 %
[167,   200] loss: 0.009402898
[167,   400] loss: 0.007800348
[167,   600] loss: 0.007019104
[167,   800] loss: 0.007349790
[167,  1000] loss: 0.007794726
[167,  1200] loss: 0.006856063
[167,  1400] loss: 0.006006959
[167,  1600] loss: 0.007065588
[167,  1800] loss: 0.005640096
[167,  2000] loss: 0.006766478
[167,  2200] loss: 0.007408299
[167,  2400] loss: 0.007464057
Final Summary:   loss: 0.072
[168,   200] loss: 0.006702343
[168,   400] loss: 0.005186787
[168,   600] loss: 0.006077777
[168,   800] loss: 0.006911962
[168,  1000] loss: 0.006074843
[168,  1200] loss: 0.006118738
[168,  1400] loss: 0.007753961
[168,  1600] loss: 0.008181475
[168,  1800] loss: 0.007231040
[168,  2000] loss: 0.008662196
[168,  2200] loss: 0.010156493
[168,  2400] loss: 0.008108544
Final Summary:   loss: 0.074
[169,   200] loss: 0.010558836
[169,   400] loss: 0.007588401
[169,   600] loss: 0.006929356
[169,   800] loss: 0.006202031
[169,  1000] loss: 0.007287359
[169,  1200] loss: 0.007039974
[169,  1400] loss: 0.008091879
[169,  1600] loss: 0.006376949
[169,  1800] loss: 0.008649467
[169,  2000] loss: 0.009017989
[169,  2200] loss: 0.008125662
[169,  2400] loss: 0.007726732
Final Summary:   loss: 0.079
[170,   200] loss: 0.009719131
[170,   400] loss: 0.006841042
[170,   600] loss: 0.008195480
[170,   800] loss: 0.007463200
[170,  1000] loss: 0.005709542
[170,  1200] loss: 0.006497897
[170,  1400] loss: 0.007359671
[170,  1600] loss: 0.007639155
[170,  1800] loss: 0.007009376
[170,  2000] loss: 0.005970242
[170,  2200] loss: 0.006631164
[170,  2400] loss: 0.005913093
Final Summary:   loss: 0.071
[171,   200] loss: 0.007725332
[171,   400] loss: 0.006157523
[171,   600] loss: 0.006552378
[171,   800] loss: 0.007289686
[171,  1000] loss: 0.007381761
[171,  1200] loss: 0.006506130
[171,  1400] loss: 0.007313033
[171,  1600] loss: 0.007075223
[171,  1800] loss: 0.007927065
[171,  2000] loss: 0.006186742
[171,  2200] loss: 0.007117112
[171,  2400] loss: 0.005329622
Final Summary:   loss: 0.069
Train Accuracy of the network: 98 %
[172,   200] loss: 0.006050928
[172,   400] loss: 0.005630054
[172,   600] loss: 0.007016309
[172,   800] loss: 0.006323330
[172,  1000] loss: 0.005313915
[172,  1200] loss: 0.004316585
[172,  1400] loss: 0.004856510
[172,  1600] loss: 0.006885380
[172,  1800] loss: 0.007001184
[172,  2000] loss: 0.009377702
[172,  2200] loss: 0.008438131
[172,  2400] loss: 0.008118245
Final Summary:   loss: 0.067
[173,   200] loss: 0.008585889
[173,   400] loss: 0.006324549
[173,   600] loss: 0.005133507
[173,   800] loss: 0.004438747
[173,  1000] loss: 0.005546600
[173,  1200] loss: 0.004822206
[173,  1400] loss: 0.006015332
[173,  1600] loss: 0.006590206
[173,  1800] loss: 0.006004079
[173,  2000] loss: 0.006222149
[173,  2200] loss: 0.006730299
[173,  2400] loss: 0.007065662
Final Summary:   loss: 0.062
[174,   200] loss: 0.009177679
[174,   400] loss: 0.006239699
[174,   600] loss: 0.005790594
[174,   800] loss: 0.007401683
[174,  1000] loss: 0.007826430
[174,  1200] loss: 0.007826620
[174,  1400] loss: 0.007244416
[174,  1600] loss: 0.006673436
[174,  1800] loss: 0.005844723
[174,  2000] loss: 0.007432654
[174,  2200] loss: 0.007402229
[174,  2400] loss: 0.007664745
Final Summary:   loss: 0.073
[175,   200] loss: 0.009761890
[175,   400] loss: 0.007074642
[175,   600] loss: 0.006138650
[175,   800] loss: 0.007408110
[175,  1000] loss: 0.006762018
[175,  1200] loss: 0.008850913
[175,  1400] loss: 0.007886321
[175,  1600] loss: 0.006914206
[175,  1800] loss: 0.007608433
[175,  2000] loss: 0.008521112
[175,  2200] loss: 0.008288848
[175,  2400] loss: 0.008118867
Final Summary:   loss: 0.078
[176,   200] loss: 0.006098375
[176,   400] loss: 0.005967154
[176,   600] loss: 0.006661422
[176,   800] loss: 0.006150415
[176,  1000] loss: 0.007024973
[176,  1200] loss: 0.006270640
[176,  1400] loss: 0.006205863
[176,  1600] loss: 0.005356504
[176,  1800] loss: 0.006936580
[176,  2000] loss: 0.005320646
[176,  2200] loss: 0.009766124
[176,  2400] loss: 0.008609390
Final Summary:   loss: 0.068
Train Accuracy of the network: 78 %
[177,   200] loss: 0.010379923
[177,   400] loss: 0.005438359
[177,   600] loss: 0.006592039
[177,   800] loss: 0.007931533
[177,  1000] loss: 0.007117208
[177,  1200] loss: 0.006336757
[177,  1400] loss: 0.005328327
[177,  1600] loss: 0.005898384
[177,  1800] loss: 0.008870546
[177,  2000] loss: 0.006903969
[177,  2200] loss: 0.007818828
[177,  2400] loss: 0.008004586
Final Summary:   loss: 0.072
[178,   200] loss: 0.007461514
[178,   400] loss: 0.008234258
[178,   600] loss: 0.005894393
[178,   800] loss: 0.007293833
[178,  1000] loss: 0.006913559
[178,  1200] loss: 0.007108307
[178,  1400] loss: 0.006937903
[178,  1600] loss: 0.006565452
[178,  1800] loss: 0.006328236
[178,  2000] loss: 0.006540155
[178,  2200] loss: 0.006626466
[178,  2400] loss: 0.007975820
Final Summary:   loss: 0.070
[179,   200] loss: 0.007221868
[179,   400] loss: 0.006103557
[179,   600] loss: 0.008199105
[179,   800] loss: 0.006577267
[179,  1000] loss: 0.006976870
[179,  1200] loss: 0.007278355
[179,  1400] loss: 0.006069234
[179,  1600] loss: 0.006723211
[179,  1800] loss: 0.006178628
[179,  2000] loss: 0.007415462
[179,  2200] loss: 0.006710685
[179,  2400] loss: 0.006553149
Final Summary:   loss: 0.069
[180,   200] loss: 0.008103472
[180,   400] loss: 0.004447808
[180,   600] loss: 0.005680263
[180,   800] loss: 0.008416762
[180,  1000] loss: 0.007677799
[180,  1200] loss: 0.007129360
[180,  1400] loss: 0.006749893
[180,  1600] loss: 0.007961541
[180,  1800] loss: 0.006995585
[180,  2000] loss: 0.006528668
[180,  2200] loss: 0.006196809
[180,  2400] loss: 0.005914865
Final Summary:   loss: 0.069
[181,   200] loss: 0.008744043
[181,   400] loss: 0.005729010
[181,   600] loss: 0.004543992
[181,   800] loss: 0.005692390
[181,  1000] loss: 0.005886482
[181,  1200] loss: 0.006158325
[181,  1400] loss: 0.006874303
[181,  1600] loss: 0.006032691
[181,  1800] loss: 0.006080961
[181,  2000] loss: 0.006256852
[181,  2200] loss: 0.007209628
[181,  2400] loss: 0.005303377
Final Summary:   loss: 0.062
Train Accuracy of the network: 98 %
[182,   200] loss: 0.004713193
[182,   400] loss: 0.004679410
[182,   600] loss: 0.005167755
[182,   800] loss: 0.005516200
[182,  1000] loss: 0.005037191
[182,  1200] loss: 0.004914960
[182,  1400] loss: 0.005341133
[182,  1600] loss: 0.004655210
[182,  1800] loss: 0.005700925
[182,  2000] loss: 0.006291030
[182,  2200] loss: 0.006607107
[182,  2400] loss: 0.006158481
Final Summary:   loss: 0.055
[183,   200] loss: 0.010632154
[183,   400] loss: 0.005691379
[183,   600] loss: 0.008123368
[183,   800] loss: 0.007725156
[183,  1000] loss: 0.007747733
[183,  1200] loss: 0.005769506
[183,  1400] loss: 0.006045602
[183,  1600] loss: 0.005705589
[183,  1800] loss: 0.005917814
[183,  2000] loss: 0.005683355
[183,  2200] loss: 0.007935811
[183,  2400] loss: 0.006040082
Final Summary:   loss: 0.071
[184,   200] loss: 0.009835890
[184,   400] loss: 0.008290127
[184,   600] loss: 0.006949776
[184,   800] loss: 0.006891554
[184,  1000] loss: 0.007703512
[184,  1200] loss: 0.006770692
[184,  1400] loss: 0.007238928
[184,  1600] loss: 0.007002630
[184,  1800] loss: 0.005867568
[184,  2000] loss: 0.005452762
[184,  2200] loss: 0.005971792
[184,  2400] loss: 0.005444759
Final Summary:   loss: 0.070
[185,   200] loss: 0.005866773
[185,   400] loss: 0.004594157
[185,   600] loss: 0.006059333
[185,   800] loss: 0.005297834
[185,  1000] loss: 0.005652819
[185,  1200] loss: 0.006030356
[185,  1400] loss: 0.005797941
[185,  1600] loss: 0.004088987
[185,  1800] loss: 0.005137865
[185,  2000] loss: 0.006421610
[185,  2200] loss: 0.006231701
[185,  2400] loss: 0.006990821
Final Summary:   loss: 0.058
[186,   200] loss: 0.014044292
[186,   400] loss: 0.008572412
[186,   600] loss: 0.005654665
[186,   800] loss: 0.006665585
[186,  1000] loss: 0.006047960
[186,  1200] loss: 0.006039206
[186,  1400] loss: 0.005906268
[186,  1600] loss: 0.004824310
[186,  1800] loss: 0.004733012
[186,  2000] loss: 0.005791214
[186,  2200] loss: 0.007121876
[186,  2400] loss: 0.006088889
Final Summary:   loss: 0.068
Train Accuracy of the network: 95 %
[187,   200] loss: 0.005644442
[187,   400] loss: 0.006077569
[187,   600] loss: 0.004997173
[187,   800] loss: 0.006062824
[187,  1000] loss: 0.005627172
[187,  1200] loss: 0.006092668
[187,  1400] loss: 0.006232377
[187,  1600] loss: 0.006547945
[187,  1800] loss: 0.007846051
[187,  2000] loss: 0.006927977
[187,  2200] loss: 0.006644142
[187,  2400] loss: 0.005472931
Final Summary:   loss: 0.063
[188,   200] loss: 0.007983379
[188,   400] loss: 0.004896988
[188,   600] loss: 0.004074732
[188,   800] loss: 0.005207455
[188,  1000] loss: 0.004769471
[188,  1200] loss: 0.005707920
[188,  1400] loss: 0.005189927
[188,  1600] loss: 0.005848391
[188,  1800] loss: 0.006880697
[188,  2000] loss: 0.006594975
[188,  2200] loss: 0.006013524
[188,  2400] loss: 0.007148601
Final Summary:   loss: 0.059
[189,   200] loss: 0.004009295
[189,   400] loss: 0.003364345
[189,   600] loss: 0.003395288
[189,   800] loss: 0.004326333
[189,  1000] loss: 0.003715295
[189,  1200] loss: 0.003398690
[189,  1400] loss: 0.005160245
[189,  1600] loss: 0.005377592
[189,  1800] loss: 0.005542539
[189,  2000] loss: 0.005183642
[189,  2200] loss: 0.004756466
[189,  2400] loss: 0.004319753
Final Summary:   loss: 0.045
[190,   200] loss: 0.010065122
[190,   400] loss: 0.005527264
[190,   600] loss: 0.005662244
[190,   800] loss: 0.005842231
[190,  1000] loss: 0.005032588
[190,  1200] loss: 0.005095156
[190,  1400] loss: 0.005297104
[190,  1600] loss: 0.004202694
[190,  1800] loss: 0.004342936
[190,  2000] loss: 0.006124597
[190,  2200] loss: 0.006226779
[190,  2400] loss: 0.005685441
Final Summary:   loss: 0.058
[191,   200] loss: 0.007684540
[191,   400] loss: 0.004327722
[191,   600] loss: 0.005947045
[191,   800] loss: 0.005373695
[191,  1000] loss: 0.004236198
[191,  1200] loss: 0.005326127
[191,  1400] loss: 0.005155893
[191,  1600] loss: 0.006318649
[191,  1800] loss: 0.005071240
[191,  2000] loss: 0.005333991
[191,  2200] loss: 0.005510642
[191,  2400] loss: 0.005887754
Final Summary:   loss: 0.056
Train Accuracy of the network: 82 %
[192,   200] loss: 0.009859714
[192,   400] loss: 0.006549842
[192,   600] loss: 0.004990346
[192,   800] loss: 0.006479911
[192,  1000] loss: 0.004950057
[192,  1200] loss: 0.006034082
[192,  1400] loss: 0.005223120
[192,  1600] loss: 0.004540687
[192,  1800] loss: 0.004127767
[192,  2000] loss: 0.005269816
[192,  2200] loss: 0.005338437
[192,  2400] loss: 0.005901535
Final Summary:   loss: 0.058
[193,   200] loss: 0.003814001
[193,   400] loss: 0.005599153
[193,   600] loss: 0.005771480
[193,   800] loss: 0.003520033
[193,  1000] loss: 0.004710254
[193,  1200] loss: 0.006395806
[193,  1400] loss: 0.004240525
[193,  1600] loss: 0.004632706
[193,  1800] loss: 0.004778759
[193,  2000] loss: 0.004864027
[193,  2200] loss: 0.004964172
[193,  2400] loss: 0.005693565
Final Summary:   loss: 0.049
[194,   200] loss: 0.004876997
[194,   400] loss: 0.003469887
[194,   600] loss: 0.005048273
[194,   800] loss: 0.003887918
[194,  1000] loss: 0.005551523
[194,  1200] loss: 0.003724224
[194,  1400] loss: 0.003735811
[194,  1600] loss: 0.003654265
[194,  1800] loss: 0.005420003
[194,  2000] loss: 0.006171185
[194,  2200] loss: 0.006251391
[194,  2400] loss: 0.005994976
Final Summary:   loss: 0.049
[195,   200] loss: 0.010142622
[195,   400] loss: 0.004731311
[195,   600] loss: 0.004518871
[195,   800] loss: 0.004723375
[195,  1000] loss: 0.004289686
[195,  1200] loss: 0.004488217
[195,  1400] loss: 0.005762452
[195,  1600] loss: 0.004286988
[195,  1800] loss: 0.004324052
[195,  2000] loss: 0.004804737
[195,  2200] loss: 0.004752718
[195,  2400] loss: 0.004614631
Final Summary:   loss: 0.052
[196,   200] loss: 0.006718240
[196,   400] loss: 0.004294542
[196,   600] loss: 0.004374337
[196,   800] loss: 0.004228120
[196,  1000] loss: 0.004401125
[196,  1200] loss: 0.003639800
[196,  1400] loss: 0.003650054
[196,  1600] loss: 0.004494404
[196,  1800] loss: 0.003772619
[196,  2000] loss: 0.002905675
[196,  2200] loss: 0.003705351
[196,  2400] loss: 0.003454091
Final Summary:   loss: 0.041
Train Accuracy of the network: 97 %
[197,   200] loss: 0.004008611
[197,   400] loss: 0.004910251
[197,   600] loss: 0.004697951
[197,   800] loss: 0.004793862
[197,  1000] loss: 0.005110785
[197,  1200] loss: 0.004470427
[197,  1400] loss: 0.003981147
[197,  1600] loss: 0.004148464
[197,  1800] loss: 0.003851470
[197,  2000] loss: 0.003831103
[197,  2200] loss: 0.003959921
[197,  2400] loss: 0.005047714
Final Summary:   loss: 0.045
[198,   200] loss: 0.005593170
[198,   400] loss: 0.004205397
[198,   600] loss: 0.003745332
[198,   800] loss: 0.005343211
[198,  1000] loss: 0.003776763
[198,  1200] loss: 0.004424975
[198,  1400] loss: 0.004011368
[198,  1600] loss: 0.005480267
[198,  1800] loss: 0.005062877
[198,  2000] loss: 0.005602100
[198,  2200] loss: 0.005136364
[198,  2400] loss: 0.004008647
Final Summary:   loss: 0.048
[199,   200] loss: 0.009571610
[199,   400] loss: 0.005104413
[199,   600] loss: 0.006408098
[199,   800] loss: 0.004364592
[199,  1000] loss: 0.004094116
[199,  1200] loss: 0.004322755
[199,  1400] loss: 0.003709427
[199,  1600] loss: 0.004966277
[199,  1800] loss: 0.004158701
[199,  2000] loss: 0.003883517
[199,  2200] loss: 0.005467902
[199,  2400] loss: 0.003686049
Final Summary:   loss: 0.050
[200,   200] loss: 0.005778131
[200,   400] loss: 0.004780239
[200,   600] loss: 0.003751476
[200,   800] loss: 0.003748076
[200,  1000] loss: 0.003033600
[200,  1200] loss: 0.003952535
[200,  1400] loss: 0.003803238
[200,  1600] loss: 0.003864772
[200,  1800] loss: 0.003617590
[200,  2000] loss: 0.003743696
[200,  2200] loss: 0.003751291
[200,  2400] loss: 0.003232207
Final Summary:   loss: 0.039
[201,   200] loss: 0.004029941
[201,   400] loss: 0.004908009
[201,   600] loss: 0.003907772
[201,   800] loss: 0.004320626
[201,  1000] loss: 0.004853498
[201,  1200] loss: 0.004342004
[201,  1400] loss: 0.005032515
[201,  1600] loss: 0.004133323
[201,  1800] loss: 0.005807901
[201,  2000] loss: 0.003875000
[201,  2200] loss: 0.004799702
[201,  2400] loss: 0.003954273
Final Summary:   loss: 0.046
Train Accuracy of the network: 75 %
[202,   200] loss: 0.006347749
[202,   400] loss: 0.005138492
[202,   600] loss: 0.005729996
[202,   800] loss: 0.006580770
[202,  1000] loss: 0.004057890
[202,  1200] loss: 0.003811113
[202,  1400] loss: 0.003934926
[202,  1600] loss: 0.004232789
[202,  1800] loss: 0.003422370
[202,  2000] loss: 0.005276711
[202,  2200] loss: 0.004080933
[202,  2400] loss: 0.005259136
Final Summary:   loss: 0.050
[203,   200] loss: 0.008642629
[203,   400] loss: 0.006279813
[203,   600] loss: 0.004302950
[203,   800] loss: 0.002746295
[203,  1000] loss: 0.004279032
[203,  1200] loss: 0.005650365
[203,  1400] loss: 0.004968620
[203,  1600] loss: 0.005301229
[203,  1800] loss: 0.004671051
[203,  2000] loss: 0.004304025
[203,  2200] loss: 0.004575131
[203,  2400] loss: 0.005163574
Final Summary:   loss: 0.051
[204,   200] loss: 0.006965212
[204,   400] loss: 0.005471521
[204,   600] loss: 0.004807571
[204,   800] loss: 0.005456710
[204,  1000] loss: 0.005587508
[204,  1200] loss: 0.005058445
[204,  1400] loss: 0.004498743
[204,  1600] loss: 0.005773210
[204,  1800] loss: 0.004972375
[204,  2000] loss: 0.005025261
[204,  2200] loss: 0.004562997
[204,  2400] loss: 0.005643947
Final Summary:   loss: 0.053
[205,   200] loss: 0.004953933
[205,   400] loss: 0.005250791
[205,   600] loss: 0.003591343
[205,   800] loss: 0.004302849
[205,  1000] loss: 0.004085172
[205,  1200] loss: 0.003838157
[205,  1400] loss: 0.003830373
[205,  1600] loss: 0.004626616
[205,  1800] loss: 0.005068946
[205,  2000] loss: 0.003751346
[205,  2200] loss: 0.004940554
[205,  2400] loss: 0.003701042
Final Summary:   loss: 0.044
[206,   200] loss: 0.006280527
[206,   400] loss: 0.003048928
[206,   600] loss: 0.006054384
[206,   800] loss: 0.004772377
[206,  1000] loss: 0.005667263
[206,  1200] loss: 0.003861115
[206,  1400] loss: 0.004816378
[206,  1600] loss: 0.004113765
[206,  1800] loss: 0.002957540
[206,  2000] loss: 0.004601436
[206,  2200] loss: 0.003876860
[206,  2400] loss: 0.004819628
Final Summary:   loss: 0.046
Train Accuracy of the network: 98 %
[207,   200] loss: 0.005521701
[207,   400] loss: 0.004602595
[207,   600] loss: 0.006378282
[207,   800] loss: 0.003815071
[207,  1000] loss: 0.004549028
[207,  1200] loss: 0.004128669
[207,  1400] loss: 0.004115627
[207,  1600] loss: 0.003805865
[207,  1800] loss: 0.003467271
[207,  2000] loss: 0.003566530
[207,  2200] loss: 0.005028919
[207,  2400] loss: 0.005078797
Final Summary:   loss: 0.046
[208,   200] loss: 0.007133412
[208,   400] loss: 0.004275998
[208,   600] loss: 0.006097151
[208,   800] loss: 0.004456565
[208,  1000] loss: 0.004109345
[208,  1200] loss: 0.004532486
[208,  1400] loss: 0.003503403
[208,  1600] loss: 0.004698663
[208,  1800] loss: 0.005694151
[208,  2000] loss: 0.005737837
[208,  2200] loss: 0.005304705
[208,  2400] loss: 0.005650718
Final Summary:   loss: 0.051
[209,   200] loss: 0.006583924
[209,   400] loss: 0.003697012
[209,   600] loss: 0.005493129
[209,   800] loss: 0.005356478
[209,  1000] loss: 0.004494237
[209,  1200] loss: 0.003465793
[209,  1400] loss: 0.005101988
[209,  1600] loss: 0.003925817
[209,  1800] loss: 0.003996139
[209,  2000] loss: 0.004788961
[209,  2200] loss: 0.004533279
[209,  2400] loss: 0.005315540
Final Summary:   loss: 0.049
[210,   200] loss: 0.010797365
[210,   400] loss: 0.005891279
[210,   600] loss: 0.005489755
[210,   800] loss: 0.005934852
[210,  1000] loss: 0.005627610
[210,  1200] loss: 0.006017542
[210,  1400] loss: 0.005018008
[210,  1600] loss: 0.004027456
[210,  1800] loss: 0.005884758
[210,  2000] loss: 0.006432704
[210,  2200] loss: 0.004088557
[210,  2400] loss: 0.004662446
Final Summary:   loss: 0.058
[211,   200] loss: 0.005880790
[211,   400] loss: 0.004748150
[211,   600] loss: 0.004877358
[211,   800] loss: 0.004758117
[211,  1000] loss: 0.004431549
[211,  1200] loss: 0.005997222
[211,  1400] loss: 0.004837832
[211,  1600] loss: 0.007652931
[211,  1800] loss: 0.006260491
[211,  2000] loss: 0.006649112
[211,  2200] loss: 0.006457415
[211,  2400] loss: 0.004770666
Final Summary:   loss: 0.056
Train Accuracy of the network: 90 %
[212,   200] loss: 0.006252794
[212,   400] loss: 0.004730573
[212,   600] loss: 0.004629893
[212,   800] loss: 0.004641749
[212,  1000] loss: 0.004206700
[212,  1200] loss: 0.003270754
[212,  1400] loss: 0.002981830
[212,  1600] loss: 0.003404835
[212,  1800] loss: 0.004641140
[212,  2000] loss: 0.004043324
[212,  2200] loss: 0.004810746
[212,  2400] loss: 0.004610787
Final Summary:   loss: 0.044
[213,   200] loss: 0.004574145
[213,   400] loss: 0.003116040
[213,   600] loss: 0.003704941
[213,   800] loss: 0.004045492
[213,  1000] loss: 0.003493130
[213,  1200] loss: 0.004457104
[213,  1400] loss: 0.003901860
[213,  1600] loss: 0.003671091
[213,  1800] loss: 0.003016809
[213,  2000] loss: 0.002719386
[213,  2200] loss: 0.004204353
[213,  2400] loss: 0.004932681
Final Summary:   loss: 0.039
[214,   200] loss: 0.008525595
[214,   400] loss: 0.003491911
[214,   600] loss: 0.005235881
[214,   800] loss: 0.005150499
[214,  1000] loss: 0.005722336
[214,  1200] loss: 0.004556742
[214,  1400] loss: 0.004563347
[214,  1600] loss: 0.004513887
[214,  1800] loss: 0.006497769
[214,  2000] loss: 0.005384957
[214,  2200] loss: 0.005172434
[214,  2400] loss: 0.004386200
Final Summary:   loss: 0.054
[215,   200] loss: 0.006530811
[215,   400] loss: 0.004672379
[215,   600] loss: 0.003279102
[215,   800] loss: 0.004811763
[215,  1000] loss: 0.004217621
[215,  1200] loss: 0.005678851
[215,  1400] loss: 0.004255890
[215,  1600] loss: 0.003987849
[215,  1800] loss: 0.004841535
[215,  2000] loss: 0.003514718
[215,  2200] loss: 0.004270836
[215,  2400] loss: 0.005099075
Final Summary:   loss: 0.047
[216,   200] loss: 0.007416171
[216,   400] loss: 0.004812269
[216,   600] loss: 0.003567750
[216,   800] loss: 0.004937925
[216,  1000] loss: 0.002870497
[216,  1200] loss: 0.005196700
[216,  1400] loss: 0.004908192
[216,  1600] loss: 0.003890595
[216,  1800] loss: 0.004597935
[216,  2000] loss: 0.003769974
[216,  2200] loss: 0.004362829
[216,  2400] loss: 0.004760797
Final Summary:   loss: 0.047
Train Accuracy of the network: 82 %
[217,   200] loss: 0.009200597
[217,   400] loss: 0.004402926
[217,   600] loss: 0.005851766
[217,   800] loss: 0.003656738
[217,  1000] loss: 0.004919024
[217,  1200] loss: 0.004343673
[217,  1400] loss: 0.005757297
[217,  1600] loss: 0.004169703
[217,  1800] loss: 0.004717671
[217,  2000] loss: 0.004387102
[217,  2200] loss: 0.004717437
[217,  2400] loss: 0.005501957
Final Summary:   loss: 0.052
[218,   200] loss: 0.008893192
[218,   400] loss: 0.006797890
[218,   600] loss: 0.005033391
[218,   800] loss: 0.004642643
[218,  1000] loss: 0.005150048
[218,  1200] loss: 0.004825235
[218,  1400] loss: 0.004068987
[218,  1600] loss: 0.004196199
[218,  1800] loss: 0.004227102
[218,  2000] loss: 0.003841176
[218,  2200] loss: 0.005183765
[218,  2400] loss: 0.005181997
Final Summary:   loss: 0.053
[219,   200] loss: 0.011731043
[219,   400] loss: 0.005363574
[219,   600] loss: 0.004313624
[219,   800] loss: 0.003354803
[219,  1000] loss: 0.004040891
[219,  1200] loss: 0.004576934
[219,  1400] loss: 0.004696150
[219,  1600] loss: 0.005446473
[219,  1800] loss: 0.005179302
[219,  2000] loss: 0.005753988
[219,  2200] loss: 0.004819487
[219,  2400] loss: 0.003841406
Final Summary:   loss: 0.053
[220,   200] loss: 0.007362055
[220,   400] loss: 0.004911475
[220,   600] loss: 0.004181713
[220,   800] loss: 0.005138854
[220,  1000] loss: 0.005547493
[220,  1200] loss: 0.005435655
[220,  1400] loss: 0.003548406
[220,  1600] loss: 0.004883416
[220,  1800] loss: 0.004279088
[220,  2000] loss: 0.004202723
[220,  2200] loss: 0.004695681
[220,  2400] loss: 0.004027286
Final Summary:   loss: 0.049
[221,   200] loss: 0.005684369
[221,   400] loss: 0.003108069
[221,   600] loss: 0.003571211
[221,   800] loss: 0.003950061
[221,  1000] loss: 0.004326276
[221,  1200] loss: 0.002957000
[221,  1400] loss: 0.002727707
[221,  1600] loss: 0.003757460
[221,  1800] loss: 0.003975838
[221,  2000] loss: 0.003488645
[221,  2200] loss: 0.005110341
[221,  2400] loss: 0.005113178
Final Summary:   loss: 0.041
Train Accuracy of the network: 80 %
[222,   200] loss: 0.008409987
[222,   400] loss: 0.005178230
[222,   600] loss: 0.004607656
[222,   800] loss: 0.005783476
[222,  1000] loss: 0.003716363
[222,  1200] loss: 0.004525549
[222,  1400] loss: 0.003931279
[222,  1600] loss: 0.003221458
[222,  1800] loss: 0.003289826
[222,  2000] loss: 0.003741254
[222,  2200] loss: 0.005940143
[222,  2400] loss: 0.004487745
Final Summary:   loss: 0.048
[223,   200] loss: 0.003851140
[223,   400] loss: 0.004193602
[223,   600] loss: 0.003768555
[223,   800] loss: 0.003099973
[223,  1000] loss: 0.003442702
[223,  1200] loss: 0.003364518
[223,  1400] loss: 0.003765077
[223,  1600] loss: 0.002650872
[223,  1800] loss: 0.003513965
[223,  2000] loss: 0.002917234
[223,  2200] loss: 0.002646510
[223,  2400] loss: 0.004117654
Final Summary:   loss: 0.035
[224,   200] loss: 0.005471554
[224,   400] loss: 0.005305519
[224,   600] loss: 0.004780796
[224,   800] loss: 0.003943030
[224,  1000] loss: 0.004010452
[224,  1200] loss: 0.004383162
[224,  1400] loss: 0.004272380
[224,  1600] loss: 0.004340429
[224,  1800] loss: 0.003389729
[224,  2000] loss: 0.003113692
[224,  2200] loss: 0.002690592
[224,  2400] loss: 0.002786420
Final Summary:   loss: 0.041
[225,   200] loss: 0.004418638
[225,   400] loss: 0.002577148
[225,   600] loss: 0.003064183
[225,   800] loss: 0.002793646
[225,  1000] loss: 0.003031041
[225,  1200] loss: 0.003818955
[225,  1400] loss: 0.003814022
[225,  1600] loss: 0.003523546
[225,  1800] loss: 0.003184567
[225,  2000] loss: 0.003024521
[225,  2200] loss: 0.003259213
[225,  2400] loss: 0.003402794
Final Summary:   loss: 0.034
[226,   200] loss: 0.006083685
[226,   400] loss: 0.003852856
[226,   600] loss: 0.003056986
[226,   800] loss: 0.002976564
[226,  1000] loss: 0.004715358
[226,  1200] loss: 0.003205847
[226,  1400] loss: 0.002348998
[226,  1600] loss: 0.003513858
[226,  1800] loss: 0.003917050
[226,  2000] loss: 0.002944361
[226,  2200] loss: 0.002094897
[226,  2400] loss: 0.002739856
Final Summary:   loss: 0.036
Train Accuracy of the network: 79 %
[227,   200] loss: 0.008336273
[227,   400] loss: 0.005389471
[227,   600] loss: 0.003885250
[227,   800] loss: 0.004161467
[227,  1000] loss: 0.004677602
[227,  1200] loss: 0.003547236
[227,  1400] loss: 0.003599638
[227,  1600] loss: 0.004634196
[227,  1800] loss: 0.004069895
[227,  2000] loss: 0.003532444
[227,  2200] loss: 0.003525594
[227,  2400] loss: 0.002629101
Final Summary:   loss: 0.044
[228,   200] loss: 0.005018027
[228,   400] loss: 0.004090113
[228,   600] loss: 0.003035149
[228,   800] loss: 0.003801926
[228,  1000] loss: 0.002897485
[228,  1200] loss: 0.003154620
[228,  1400] loss: 0.002539455
[228,  1600] loss: 0.003410532
[228,  1800] loss: 0.003808948
[228,  2000] loss: 0.003164833
[228,  2200] loss: 0.003732481
[228,  2400] loss: 0.003184692
Final Summary:   loss: 0.035
[229,   200] loss: 0.003153001
[229,   400] loss: 0.003543166
[229,   600] loss: 0.002973618
[229,   800] loss: 0.002747094
[229,  1000] loss: 0.003085738
[229,  1200] loss: 0.003472754
[229,  1400] loss: 0.004590053
[229,  1600] loss: 0.002995418
[229,  1800] loss: 0.005086671
[229,  2000] loss: 0.003249237
[229,  2200] loss: 0.002432433
[229,  2400] loss: 0.002691479
Final Summary:   loss: 0.035
[230,   200] loss: 0.008323966
[230,   400] loss: 0.004181315
[230,   600] loss: 0.003710247
[230,   800] loss: 0.004124937
[230,  1000] loss: 0.003751729
[230,  1200] loss: 0.003787146
[230,  1400] loss: 0.004766321
[230,  1600] loss: 0.003817037
[230,  1800] loss: 0.003673546
[230,  2000] loss: 0.003709636
[230,  2200] loss: 0.003717509
[230,  2400] loss: 0.003833963
Final Summary:   loss: 0.043
[231,   200] loss: 0.004454422
[231,   400] loss: 0.003257407
[231,   600] loss: 0.003405634
[231,   800] loss: 0.002449272
[231,  1000] loss: 0.003317753
[231,  1200] loss: 0.005024431
[231,  1400] loss: 0.003425953
[231,  1600] loss: 0.002971036
[231,  1800] loss: 0.003340279
[231,  2000] loss: 0.003729322
[231,  2200] loss: 0.003360134
[231,  2400] loss: 0.004410310
Final Summary:   loss: 0.038
Train Accuracy of the network: 62 %
[232,   200] loss: 0.009812066
[232,   400] loss: 0.004704336
[232,   600] loss: 0.003929548
[232,   800] loss: 0.003920870
[232,  1000] loss: 0.003075960
[232,  1200] loss: 0.003877145
[232,  1400] loss: 0.003110598
[232,  1600] loss: 0.003304581
[232,  1800] loss: 0.003494520
[232,  2000] loss: 0.003578682
[232,  2200] loss: 0.003508265
[232,  2400] loss: 0.002871228
Final Summary:   loss: 0.041
[233,   200] loss: 0.001956104
[233,   400] loss: 0.002536878
[233,   600] loss: 0.003047729
[233,   800] loss: 0.002098426
[233,  1000] loss: 0.002723910
[233,  1200] loss: 0.003021885
[233,  1400] loss: 0.003067169
[233,  1600] loss: 0.002916725
[233,  1800] loss: 0.002753301
[233,  2000] loss: 0.003256722
[233,  2200] loss: 0.005377179
[233,  2400] loss: 0.003415723
Final Summary:   loss: 0.031
[234,   200] loss: 0.005828843
[234,   400] loss: 0.005012681
[234,   600] loss: 0.002693948
[234,   800] loss: 0.002855161
[234,  1000] loss: 0.002550216
[234,  1200] loss: 0.002723847
[234,  1400] loss: 0.002177942
[234,  1600] loss: 0.002841459
[234,  1800] loss: 0.003467676
[234,  2000] loss: 0.002624642
[234,  2200] loss: 0.003615326
[234,  2400] loss: 0.003232188
Final Summary:   loss: 0.033
[235,   200] loss: 0.002634984
[235,   400] loss: 0.002998806
[235,   600] loss: 0.002625815
[235,   800] loss: 0.002252539
[235,  1000] loss: 0.002587539
[235,  1200] loss: 0.002475982
[235,  1400] loss: 0.002821160
[235,  1600] loss: 0.002968962
[235,  1800] loss: 0.003840463
[235,  2000] loss: 0.003016698
[235,  2200] loss: 0.003061500
[235,  2400] loss: 0.002809659
Final Summary:   loss: 0.031
[236,   200] loss: 0.008365471
[236,   400] loss: 0.003793170
[236,   600] loss: 0.005657246
[236,   800] loss: 0.004938862
[236,  1000] loss: 0.005035920
[236,  1200] loss: 0.004928232
[236,  1400] loss: 0.004378823
[236,  1600] loss: 0.003902881
[236,  1800] loss: 0.004178906
[236,  2000] loss: 0.003200691
[236,  2200] loss: 0.003908094
[236,  2400] loss: 0.006688657
Final Summary:   loss: 0.049
Train Accuracy of the network: 98 %
[237,   200] loss: 0.003077450
[237,   400] loss: 0.003512302
[237,   600] loss: 0.003886022
[237,   800] loss: 0.003180145
[237,  1000] loss: 0.002693086
[237,  1200] loss: 0.002897584
[237,  1400] loss: 0.002622879
[237,  1600] loss: 0.002943903
[237,  1800] loss: 0.002866648
[237,  2000] loss: 0.003267090
[237,  2200] loss: 0.004177042
[237,  2400] loss: 0.003566694
Final Summary:   loss: 0.034
[238,   200] loss: 0.005590015
[238,   400] loss: 0.003468179
[238,   600] loss: 0.003706428
[238,   800] loss: 0.003337849
[238,  1000] loss: 0.003084791
[238,  1200] loss: 0.001933414
[238,  1400] loss: 0.002717464
[238,  1600] loss: 0.003260070
[238,  1800] loss: 0.003225538
[238,  2000] loss: 0.002628719
[238,  2200] loss: 0.003506197
[238,  2400] loss: 0.003499851
Final Summary:   loss: 0.037
[239,   200] loss: 0.006534376
[239,   400] loss: 0.003166182
[239,   600] loss: 0.003869668
[239,   800] loss: 0.003630295
[239,  1000] loss: 0.002650546
[239,  1200] loss: 0.002795941
[239,  1400] loss: 0.003830618
[239,  1600] loss: 0.003598960
[239,  1800] loss: 0.003252298
[239,  2000] loss: 0.003262968
[239,  2200] loss: 0.003034033
[239,  2400] loss: 0.003832602
Final Summary:   loss: 0.037
[240,   200] loss: 0.006484568
[240,   400] loss: 0.004136506
[240,   600] loss: 0.003377624
[240,   800] loss: 0.002822604
[240,  1000] loss: 0.003896514
[240,  1200] loss: 0.003011458
[240,  1400] loss: 0.003404518
[240,  1600] loss: 0.002618509
[240,  1800] loss: 0.002774854
[240,  2000] loss: 0.002891588
[240,  2200] loss: 0.003281366
[240,  2400] loss: 0.002800343
Final Summary:   loss: 0.036
[241,   200] loss: 0.005561305
[241,   400] loss: 0.002783159
[241,   600] loss: 0.003752100
[241,   800] loss: 0.004248963
[241,  1000] loss: 0.002999344
[241,  1200] loss: 0.003831855
[241,  1400] loss: 0.003331605
[241,  1600] loss: 0.003709183
[241,  1800] loss: 0.003125925
[241,  2000] loss: 0.003732916
[241,  2200] loss: 0.003792056
[241,  2400] loss: 0.002653785
Final Summary:   loss: 0.037
Train Accuracy of the network: 82 %
[242,   200] loss: 0.005762503
[242,   400] loss: 0.002991567
[242,   600] loss: 0.002644873
[242,   800] loss: 0.002533006
[242,  1000] loss: 0.002856229
[242,  1200] loss: 0.003462273
[242,  1400] loss: 0.002889992
[242,  1600] loss: 0.003344986
[242,  1800] loss: 0.002869580
[242,  2000] loss: 0.003435189
[242,  2200] loss: 0.002240824
[242,  2400] loss: 0.003227299
Final Summary:   loss: 0.033
[243,   200] loss: 0.004361232
[243,   400] loss: 0.002628767
[243,   600] loss: 0.002607670
[243,   800] loss: 0.003857925
[243,  1000] loss: 0.004220192
[243,  1200] loss: 0.002763420
[243,  1400] loss: 0.003598455
[243,  1600] loss: 0.002639403
[243,  1800] loss: 0.002581542
[243,  2000] loss: 0.002128767
[243,  2200] loss: 0.002918865
[243,  2400] loss: 0.002692932
Final Summary:   loss: 0.033
[244,   200] loss: 0.007969658
[244,   400] loss: 0.003943444
[244,   600] loss: 0.002727394
[244,   800] loss: 0.002748833
[244,  1000] loss: 0.002593216
[244,  1200] loss: 0.003469203
[244,  1400] loss: 0.002957798
[244,  1600] loss: 0.002590998
[244,  1800] loss: 0.003397770
[244,  2000] loss: 0.003164175
[244,  2200] loss: 0.002701478
[244,  2400] loss: 0.002760248
Final Summary:   loss: 0.036
[245,   200] loss: 0.006754678
[245,   400] loss: 0.004184662
[245,   600] loss: 0.003245033
[245,   800] loss: 0.004834277
[245,  1000] loss: 0.004129082
[245,  1200] loss: 0.003190205
[245,  1400] loss: 0.003600709
[245,  1600] loss: 0.003510066
[245,  1800] loss: 0.003328799
[245,  2000] loss: 0.003799530
[245,  2200] loss: 0.003609175
[245,  2400] loss: 0.002986204
Final Summary:   loss: 0.040
[246,   200] loss: 0.005618637
[246,   400] loss: 0.005261130
[246,   600] loss: 0.003834084
[246,   800] loss: 0.004145697
[246,  1000] loss: 0.003971307
[246,  1200] loss: 0.004068211
[246,  1400] loss: 0.002707086
[246,  1600] loss: 0.003117090
[246,  1800] loss: 0.003688703
[246,  2000] loss: 0.004092046
[246,  2200] loss: 0.003961246
[246,  2400] loss: 0.003388023
Final Summary:   loss: 0.040
Train Accuracy of the network: 99 %
[247,   200] loss: 0.002499991
[247,   400] loss: 0.003331624
[247,   600] loss: 0.002211968
[247,   800] loss: 0.002542105
[247,  1000] loss: 0.003023669
[247,  1200] loss: 0.003002968
[247,  1400] loss: 0.003861912
[247,  1600] loss: 0.002640274
[247,  1800] loss: 0.002619136
[247,  2000] loss: 0.003333146
[247,  2200] loss: 0.005285172
[247,  2400] loss: 0.003162963
Final Summary:   loss: 0.031
[248,   200] loss: 0.004084669
[248,   400] loss: 0.004420608
[248,   600] loss: 0.002628916
[248,   800] loss: 0.003365643
[248,  1000] loss: 0.002561704
[248,  1200] loss: 0.002385342
[248,  1400] loss: 0.003828821
[248,  1600] loss: 0.003412386
[248,  1800] loss: 0.003793238
[248,  2000] loss: 0.002658664
[248,  2200] loss: 0.003061909
[248,  2400] loss: 0.002539133
Final Summary:   loss: 0.033
[249,   200] loss: 0.004243072
