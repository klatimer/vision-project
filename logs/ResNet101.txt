[1,   200] loss: 0.671396306
[1,   400] loss: 0.632740294
[1,   600] loss: 0.626880512
[1,   800] loss: 0.619612035
[1,  1000] loss: 0.615729596
[1,  1200] loss: 0.612196297
Final Summary:   loss: 6.303
Train Accuracy of the network: 0 %
[2,   200] loss: 0.609749255
[2,   400] loss: 0.604550534
[2,   600] loss: 0.600611665
[2,   800] loss: 0.598550480
[2,  1000] loss: 0.597385908
[2,  1200] loss: 0.594870324
Final Summary:   loss: 6.014
[3,   200] loss: 0.592078339
[3,   400] loss: 0.588185906
[3,   600] loss: 0.585740146
[3,   800] loss: 0.579927237
[3,  1000] loss: 0.577487037
[3,  1200] loss: 0.574014174
Final Summary:   loss: 5.835
[4,   200] loss: 0.566371373
[4,   400] loss: 0.563796088
[4,   600] loss: 0.559893227
[4,   800] loss: 0.558466881
[4,  1000] loss: 0.556934597
[4,  1200] loss: 0.550781693
Final Summary:   loss: 5.598
[5,   200] loss: 0.545744143
[5,   400] loss: 0.539533951
[5,   600] loss: 0.537555625
[5,   800] loss: 0.536331535
[5,  1000] loss: 0.531248293
[5,  1200] loss: 0.526748187
Final Summary:   loss: 5.367
[6,   200] loss: 0.531642582
[6,   400] loss: 0.519375432
[6,   600] loss: 0.515255030
[6,   800] loss: 0.511475746
[6,  1000] loss: 0.505019502
[6,  1200] loss: 0.505395211
Final Summary:   loss: 5.153
Train Accuracy of the network: 0 %
[7,   200] loss: 0.496428059
[7,   400] loss: 0.495474067
[7,   600] loss: 0.487603000
[7,   800] loss: 0.484600648
[7,  1000] loss: 0.485030499
[7,  1200] loss: 0.476985583
Final Summary:   loss: 4.883
[8,   200] loss: 0.467847195
[8,   400] loss: 0.466942186
[8,   600] loss: 0.460376949
[8,   800] loss: 0.463268391
[8,  1000] loss: 0.462263585
[8,  1200] loss: 0.455285490
Final Summary:   loss: 4.631
[9,   200] loss: 0.446113788
[9,   400] loss: 0.440313783
[9,   600] loss: 0.440452514
[9,   800] loss: 0.432320427
[9,  1000] loss: 0.430956912
[9,  1200] loss: 0.426483254
Final Summary:   loss: 4.363
[10,   200] loss: 0.417091191
[10,   400] loss: 0.411613283
[10,   600] loss: 0.410881318
[10,   800] loss: 0.407470090
[10,  1000] loss: 0.404491911
[10,  1200] loss: 0.402359989
Final Summary:   loss: 4.095
[11,   200] loss: 0.394025621
[11,   400] loss: 0.386938760
[11,   600] loss: 0.385803133
[11,   800] loss: 0.381219559
[11,  1000] loss: 0.382395495
[11,  1200] loss: 0.382181896
Final Summary:   loss: 3.858
Train Accuracy of the network: 3 %
[12,   200] loss: 0.364663373
[12,   400] loss: 0.364928207
[12,   600] loss: 0.359487284
[12,   800] loss: 0.358097132
[12,  1000] loss: 0.360628351
[12,  1200] loss: 0.357864247
Final Summary:   loss: 3.614
[13,   200] loss: 0.343164379
[13,   400] loss: 0.340299904
[13,   600] loss: 0.339303379
[13,   800] loss: 0.340275030
[13,  1000] loss: 0.337814152
[13,  1200] loss: 0.333705766
Final Summary:   loss: 3.396
[14,   200] loss: 0.328204028
[14,   400] loss: 0.320232085
[14,   600] loss: 0.315492517
[14,   800] loss: 0.322002495
[14,  1000] loss: 0.312971608
[14,  1200] loss: 0.308570960
Final Summary:   loss: 3.183
[15,   200] loss: 0.299467913
[15,   400] loss: 0.298537751
[15,   600] loss: 0.301288636
[15,   800] loss: 0.299530705
[15,  1000] loss: 0.296989843
[15,  1200] loss: 0.298097420
Final Summary:   loss: 2.994
[16,   200] loss: 0.287622172
[16,   400] loss: 0.279897465
[16,   600] loss: 0.279959457
[16,   800] loss: 0.279043633
[16,  1000] loss: 0.280835529
[16,  1200] loss: 0.273971207
Final Summary:   loss: 2.808
Train Accuracy of the network: 10 %
[17,   200] loss: 0.267695879
[17,   400] loss: 0.261306684
[17,   600] loss: 0.270678640
[17,   800] loss: 0.261839274
[17,  1000] loss: 0.261958839
[17,  1200] loss: 0.259646594
Final Summary:   loss: 2.643
[18,   200] loss: 0.246698656
[18,   400] loss: 0.248362719
[18,   600] loss: 0.249212844
[18,   800] loss: 0.242429764
[18,  1000] loss: 0.247844871
[18,  1200] loss: 0.246637017
Final Summary:   loss: 2.470
[19,   200] loss: 0.234289645
[19,   400] loss: 0.230035149
[19,   600] loss: 0.233701735
[19,   800] loss: 0.232492533
[19,  1000] loss: 0.234005914
[19,  1200] loss: 0.232349047
Final Summary:   loss: 2.334
[20,   200] loss: 0.224866861
[20,   400] loss: 0.213055647
[20,   600] loss: 0.216471997
[20,   800] loss: 0.222677412
[20,  1000] loss: 0.216623211
[20,  1200] loss: 0.218715144
Final Summary:   loss: 2.191
[21,   200] loss: 0.199794817
[21,   400] loss: 0.202084210
[21,   600] loss: 0.204962535
[21,   800] loss: 0.204644190
[21,  1000] loss: 0.203070562
[21,  1200] loss: 0.209019466
Final Summary:   loss: 2.047
Train Accuracy of the network: 15 %
[22,   200] loss: 0.193212706
[22,   400] loss: 0.188268641
[22,   600] loss: 0.194385156
[22,   800] loss: 0.190165290
[22,  1000] loss: 0.194596943
[22,  1200] loss: 0.187702432
Final Summary:   loss: 1.922
[23,   200] loss: 0.187825103
[23,   400] loss: 0.180841989
[23,   600] loss: 0.181821052
[23,   800] loss: 0.182013162
[23,  1000] loss: 0.182438296
[23,  1200] loss: 0.177885818
Final Summary:   loss: 1.828
[24,   200] loss: 0.171203543
[24,   400] loss: 0.169238156
[24,   600] loss: 0.168225706
[24,   800] loss: 0.168862601
[24,  1000] loss: 0.173068668
[24,  1200] loss: 0.173596434
Final Summary:   loss: 1.712
[25,   200] loss: 0.160072131
[25,   400] loss: 0.159916000
[25,   600] loss: 0.158917269
[25,   800] loss: 0.158157575
[25,  1000] loss: 0.165404737
[25,  1200] loss: 0.160638320
Final Summary:   loss: 1.608
[26,   200] loss: 0.150899570
[26,   400] loss: 0.149048963
[26,   600] loss: 0.150615940
[26,   800] loss: 0.156052080
[26,  1000] loss: 0.149428179
[26,  1200] loss: 0.154205031
Final Summary:   loss: 1.521
Train Accuracy of the network: 32 %
[27,   200] loss: 0.143645470
[27,   400] loss: 0.141960410
[27,   600] loss: 0.140590545
[27,   800] loss: 0.142956511
[27,  1000] loss: 0.142493260
[27,  1200] loss: 0.145471039
Final Summary:   loss: 1.431
[28,   200] loss: 0.131956553
[28,   400] loss: 0.133410554
[28,   600] loss: 0.135494504
[28,   800] loss: 0.133402574
[28,  1000] loss: 0.134618194
[28,  1200] loss: 0.137255854
Final Summary:   loss: 1.348
[29,   200] loss: 0.130405836
[29,   400] loss: 0.122294461
[29,   600] loss: 0.124374138
[29,   800] loss: 0.127462592
[29,  1000] loss: 0.130246115
[29,  1200] loss: 0.129692306
Final Summary:   loss: 1.277
[30,   200] loss: 0.117743939
[30,   400] loss: 0.116500809
[30,   600] loss: 0.117961600
[30,   800] loss: 0.118470272
[30,  1000] loss: 0.121316213
[30,  1200] loss: 0.121768720
Final Summary:   loss: 1.193
[31,   200] loss: 0.111328901
[31,   400] loss: 0.107284474
[31,   600] loss: 0.113272729
[31,   800] loss: 0.110872517
[31,  1000] loss: 0.110072194
[31,  1200] loss: 0.115536981
Final Summary:   loss: 1.117
Train Accuracy of the network: 61 %
[32,   200] loss: 0.102756198
[32,   400] loss: 0.100752388
[32,   600] loss: 0.101100860
[32,   800] loss: 0.105220449
[32,  1000] loss: 0.105820320
[32,  1200] loss: 0.106205877
Final Summary:   loss: 1.041
[33,   200] loss: 0.101702250
[33,   400] loss: 0.094630041
[33,   600] loss: 0.098050443
[33,   800] loss: 0.100870140
[33,  1000] loss: 0.099439106
[33,  1200] loss: 0.100519578
Final Summary:   loss: 0.995
[34,   200] loss: 0.088518543
[34,   400] loss: 0.090844495
[34,   600] loss: 0.090625154
[34,   800] loss: 0.093427158
[34,  1000] loss: 0.093551658
[34,  1200] loss: 0.092994249
Final Summary:   loss: 0.923
[35,   200] loss: 0.085661888
[35,   400] loss: 0.084956060
[35,   600] loss: 0.087225395
[35,   800] loss: 0.087315881
[35,  1000] loss: 0.088289795
[35,  1200] loss: 0.087868449
Final Summary:   loss: 0.874
[36,   200] loss: 0.079032689
[36,   400] loss: 0.079978060
[36,   600] loss: 0.079263043
[36,   800] loss: 0.079352283
[36,  1000] loss: 0.083710785
[36,  1200] loss: 0.084765631
Final Summary:   loss: 0.811
Train Accuracy of the network: 78 %
[37,   200] loss: 0.070796840
[37,   400] loss: 0.072351852
[37,   600] loss: 0.072875328
[37,   800] loss: 0.074640647
[37,  1000] loss: 0.074541139
[37,  1200] loss: 0.079887682
Final Summary:   loss: 0.748
[38,   200] loss: 0.070310094
[38,   400] loss: 0.068944579
[38,   600] loss: 0.068878892
[38,   800] loss: 0.070866650
[38,  1000] loss: 0.073230444
[38,  1200] loss: 0.076916602
Final Summary:   loss: 0.720
[39,   200] loss: 0.064563181
[39,   400] loss: 0.062611242
[39,   600] loss: 0.064264924
[39,   800] loss: 0.065436073
[39,  1000] loss: 0.066080845
[39,  1200] loss: 0.068853499
Final Summary:   loss: 0.657
[40,   200] loss: 0.063976670
[40,   400] loss: 0.059872359
[40,   600] loss: 0.058720591
[40,   800] loss: 0.061342746
[40,  1000] loss: 0.064457876
[40,  1200] loss: 0.062346549
Final Summary:   loss: 0.622
[41,   200] loss: 0.058104173
[41,   400] loss: 0.056164130
[41,   600] loss: 0.056053896
[41,   800] loss: 0.058157858
[41,  1000] loss: 0.058785815
[41,  1200] loss: 0.060131831
Final Summary:   loss: 0.581
Train Accuracy of the network: 67 %
[42,   200] loss: 0.050683755
[42,   400] loss: 0.052010775
[42,   600] loss: 0.051527708
[42,   800] loss: 0.053441903
[42,  1000] loss: 0.051920401
[42,  1200] loss: 0.054859111
Final Summary:   loss: 0.525
[43,   200] loss: 0.047050391
[43,   400] loss: 0.049820882
[43,   600] loss: 0.049259890
[43,   800] loss: 0.051175556
[43,  1000] loss: 0.048057067
[43,  1200] loss: 0.051484132
Final Summary:   loss: 0.499
[44,   200] loss: 0.044434158
[44,   400] loss: 0.044598732
[44,   600] loss: 0.044554097
[44,   800] loss: 0.046075353
[44,  1000] loss: 0.046512740
[44,  1200] loss: 0.048901146
Final Summary:   loss: 0.460
[45,   200] loss: 0.041933116
[45,   400] loss: 0.039775051
[45,   600] loss: 0.041449503
[45,   800] loss: 0.043259845
[45,  1000] loss: 0.044140546
[45,  1200] loss: 0.045605082
Final Summary:   loss: 0.432
[46,   200] loss: 0.040874760
[46,   400] loss: 0.038460750
[46,   600] loss: 0.039667366
[46,   800] loss: 0.038528888
[46,  1000] loss: 0.042033173
[46,  1200] loss: 0.041222548
Final Summary:   loss: 0.405
Train Accuracy of the network: 75 %
[47,   200] loss: 0.038092438
[47,   400] loss: 0.035964923
[47,   600] loss: 0.037151735
[47,   800] loss: 0.036844548
[47,  1000] loss: 0.038687673
[47,  1200] loss: 0.037767301
Final Summary:   loss: 0.378
[48,   200] loss: 0.035941667
[48,   400] loss: 0.034396716
[48,   600] loss: 0.033106022
[48,   800] loss: 0.033663614
[48,  1000] loss: 0.033790535
[48,  1200] loss: 0.036418992
Final Summary:   loss: 0.349
[49,   200] loss: 0.033687102
[49,   400] loss: 0.031262939
[49,   600] loss: 0.031781870
[49,   800] loss: 0.031841037
[49,  1000] loss: 0.032914748
[49,  1200] loss: 0.033048004
Final Summary:   loss: 0.328
[50,   200] loss: 0.030991422
[50,   400] loss: 0.029061518
[50,   600] loss: 0.030065652
[50,   800] loss: 0.030266485
[50,  1000] loss: 0.031418929
[50,  1200] loss: 0.029855771
Final Summary:   loss: 0.307
[51,   200] loss: 0.029632128
[51,   400] loss: 0.029778395
[51,   600] loss: 0.029073106
[51,   800] loss: 0.028096956
[51,  1000] loss: 0.026552789
[51,  1200] loss: 0.027936796
Final Summary:   loss: 0.288
Train Accuracy of the network: 87 %
[52,   200] loss: 0.027123668
[52,   400] loss: 0.026838594
[52,   600] loss: 0.025338964
[52,   800] loss: 0.026639335
[52,  1000] loss: 0.024079011
[52,  1200] loss: 0.026612336
Final Summary:   loss: 0.264
[53,   200] loss: 0.024360650
[53,   400] loss: 0.024195108
[53,   600] loss: 0.024726815
[53,   800] loss: 0.024474783
[53,  1000] loss: 0.024429306
[53,  1200] loss: 0.027108792
Final Summary:   loss: 0.250
[54,   200] loss: 0.024030013
[54,   400] loss: 0.020824909
[54,   600] loss: 0.022840528
[54,   800] loss: 0.023704622
[54,  1000] loss: 0.021998700
[54,  1200] loss: 0.023882669
Final Summary:   loss: 0.231
[55,   200] loss: 0.022850282
[55,   400] loss: 0.022404478
[55,   600] loss: 0.021880534
[55,   800] loss: 0.022001933
[55,  1000] loss: 0.022605819
[55,  1200] loss: 0.021828617
Final Summary:   loss: 0.225
[56,   200] loss: 0.020540246
[56,   400] loss: 0.019604914
[56,   600] loss: 0.020640141
[56,   800] loss: 0.020350979
[56,  1000] loss: 0.020455114
[56,  1200] loss: 0.021180784
Final Summary:   loss: 0.208
Train Accuracy of the network: 93 %
[57,   200] loss: 0.018660716
[57,   400] loss: 0.017695871
[57,   600] loss: 0.018955707
[57,   800] loss: 0.018631781
[57,  1000] loss: 0.020414378
[57,  1200] loss: 0.019457620
Final Summary:   loss: 0.191
[58,   200] loss: 0.018892068
[58,   400] loss: 0.019076330
[58,   600] loss: 0.018293594
[58,   800] loss: 0.019746768
[58,  1000] loss: 0.018625568
[58,  1200] loss: 0.018382674
Final Summary:   loss: 0.191
[59,   200] loss: 0.016827915
[59,   400] loss: 0.016145467
[59,   600] loss: 0.017325596
[59,   800] loss: 0.017603764
[59,  1000] loss: 0.017572598
[59,  1200] loss: 0.017700336
Final Summary:   loss: 0.175
[60,   200] loss: 0.016368543
[60,   400] loss: 0.018264235
[60,   600] loss: 0.016602737
[60,   800] loss: 0.015726234
[60,  1000] loss: 0.017805043
[60,  1200] loss: 0.016832573
Final Summary:   loss: 0.171
[61,   200] loss: 0.015322969
[61,   400] loss: 0.015905629
[61,   600] loss: 0.015185216
[61,   800] loss: 0.016056305
[61,  1000] loss: 0.015840678
[61,  1200] loss: 0.015270865
Final Summary:   loss: 0.159
Train Accuracy of the network: 95 %
[62,   200] loss: 0.015786838
[62,   400] loss: 0.014763353
[62,   600] loss: 0.015205487
[62,   800] loss: 0.014844554
[62,  1000] loss: 0.015155384
[62,  1200] loss: 0.015470002
Final Summary:   loss: 0.155
[63,   200] loss: 0.014311543
[63,   400] loss: 0.014198464
[63,   600] loss: 0.015120986
[63,   800] loss: 0.014378534
[63,  1000] loss: 0.014988950
[63,  1200] loss: 0.014247926
Final Summary:   loss: 0.150
[64,   200] loss: 0.014403796
[64,   400] loss: 0.013732206
[64,   600] loss: 0.014486793
[64,   800] loss: 0.014650461
[64,  1000] loss: 0.014766999
[64,  1200] loss: 0.014553377
Final Summary:   loss: 0.150
[65,   200] loss: 0.014363983
[65,   400] loss: 0.013931616
[65,   600] loss: 0.013515897
[65,   800] loss: 0.013511997
[65,  1000] loss: 0.014031075
[65,  1200] loss: 0.013146408
Final Summary:   loss: 0.140
